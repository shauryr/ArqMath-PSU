{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import jsonlines\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"/data/szr207/github/transformers/examples/language-modeling/output/checkpoint-3000000\"\n",
    "ques_path = '/data/szr207/dataset/ArqMath/jsons/questions/all.ques.jsonl'\n",
    "ans_path = '/data/szr207/dataset/ArqMath/jsons/answers/all.ans.jsonl'\n",
    "topic_file_path = \"/data/szr207/dataset/ArqMath/Task1/Topics/Topics_V2.0.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723b7e3e1a354650b90df981e7037c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Topic:\n",
    "    \"\"\"\n",
    "    This class shows a topic for task 1. Each topic has an topic_id which is str, a title and question which\n",
    "    is the question body and a list of tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic_id, title, question, tags):\n",
    "        self.topic_id = topic_id\n",
    "        self.title = title\n",
    "        self.question = question\n",
    "        self.lst_tags = tags\n",
    "\n",
    "\n",
    "class TopicReader:\n",
    "    \"\"\"\n",
    "    This class takes in the topic file path and read all the topics into a map. The key in this map is the topic id\n",
    "    and the values are Topic which has 4 attributes: id, title, question and list of tags for each topic.\n",
    "\n",
    "    To see each topic, use the get_topic method, which takes the topic id and return the topic in Topic object and\n",
    "    you have access to the 4 attributes mentioned above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic_file_path):\n",
    "        self.__map_topics = self.__read_topics(topic_file_path)\n",
    "\n",
    "    def __read_topics(self, topic_file_path):\n",
    "        map_topics = {}\n",
    "        tree = ET.parse(topic_file_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            topic_id = child.attrib['number']\n",
    "            title = child[0].text\n",
    "            question = child[1].text\n",
    "            lst_tag = child[2].text.split(\",\")\n",
    "            map_topics[topic_id] = Topic(topic_id, title, question, lst_tag)\n",
    "        return map_topics\n",
    "\n",
    "    def get_topic(self, topic_id):\n",
    "        if topic_id in self.__map_topics:\n",
    "            return self.__map_topics[topic_id]\n",
    "        return None\n",
    "\n",
    "def remove_stop(query):\n",
    "    with open('englishST.txt') as f:\n",
    "        all_stopwords = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    all_stopwords = [x.strip() for x in all_stopwords] \n",
    "    text_tokens = query.split(' ')\n",
    "    query = [word for word in text_tokens if not word in all_stopwords]\n",
    "    query = ' '.join(query)\n",
    "    return query\n",
    "\n",
    "def remove_punct(my_str):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    # To take input from the user\n",
    "    # my_str = input(\"Enter a string: \")\n",
    "\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    return no_punct\n",
    "\n",
    "es = Elasticsearch(['http://csxindex05:9200/'], verify_certs=True)\n",
    "queries = []\n",
    "#\"In this example, the title and the question body of topic with id A.1 is printed.\"\n",
    "topic_reader = TopicReader(topic_file_path)\n",
    "dict_q_a = defaultdict(list)\n",
    "\n",
    "topic_list = []\n",
    "with open('../runs/qrel_task1', 'r') as eval_file:\n",
    "    for _,line in enumerate(eval_file):\n",
    "        topic_list.append(line.split('\\t')[0])\n",
    "\n",
    "topic_list = list(set(topic_list))\n",
    "\n",
    "list_p = []\n",
    "with open('../runs/qrel_task1', 'r') as eval_file:\n",
    "    for _,line in enumerate(eval_file):\n",
    "        list_p.append(line.split('\\t')[2])\n",
    "\n",
    "list_p = list(set(list_p))\n",
    "\n",
    "# with open('psu-task1-mlt.base-auto-both-A.tsv', 'w') as eval_file:\n",
    "for topic_id in tqdm(topic_list):\n",
    "#     if topic_id in topic_list:\n",
    "    title = re.sub('<[^<]+?>', '', topic_reader.get_topic(topic_id).title)\n",
    "    body = topic_reader.get_topic(topic_id).question\n",
    "    body_pro = re.sub('<[^<]+?>', '', body)\n",
    "    query = title + '. ' + body_pro\n",
    "    queries.append(query)\n",
    "    query = query.lower()\n",
    "    query = remove_stop(query)\n",
    "#     print(topic_id, query)\n",
    "    body = {\n",
    "        \"size\": 1000,\n",
    "         \"query\": {\n",
    "               \"more_like_this\" : {\n",
    "            \"fields\" : [\"body\"],\n",
    "            \"like\" : query,\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=\"arq_ans_ques\", body=body, request_timeout=1000)\n",
    "\n",
    "    for result in res['hits']['hits']:\n",
    "        if str(result['_source']['post_id']) in list_p:\n",
    "            dict_q_a[topic_id].append(result['_source']['post_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# load model\n",
    "# model = RobertaModel.from_pretrained(\"/data/szr207/github/transformers/examples/language-modeling/output/checkpoint-4000000\", output_hidden_states=True)\n",
    "# model = RobertaModel.from_pretrained(\"roberta-base\", output_hidden_states=True)\n",
    "feat_ext = pipeline(\"feature-extraction\", model=\"/data/szr207/github/transformers/examples/language-modeling/output/checkpoint-4000000\", tokenizer='roberta-base', device=0)\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a731c2fd61554bc4b0a21773f2f0f8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1472 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ques_emb = []\n",
    "SEQ_LENGTH = 512\n",
    "for query in tqdm(queries):\n",
    "    token_ids = tokenizer.encode(query)[:SEQ_LENGTH]\n",
    "    token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "    token_ids = token_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=token_ids)\n",
    "    hidden_states = out[2]\n",
    "    del out\n",
    "    torch.cuda.empty_cache()\n",
    "    sentence_embedding = torch.mean(hidden_states[-1], dim=1).squeeze()\n",
    "    ques_emb.append(sentence_embedding)\n",
    "    \n",
    "\n",
    "#mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aa0c034bec4bcbbf8ec6476eef8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1865cb08aa3a4709917ad9a44db7a8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9664e2fe621f4212befbee8cbcf3be44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1435643.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_ques = {}\n",
    "dict_ans = {}\n",
    "dict_aid_body = {}\n",
    "\n",
    "with jsonlines.open(os.path.join(ques_path)) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            dict_ques[obj['post_id']] = obj\n",
    "    \n",
    "with jsonlines.open(os.path.join(ans_path)) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            dict_ans[obj['post_id']] = obj\n",
    "\n",
    "for a_id in tqdm(list(dict_ans.keys())):\n",
    "    ans_body = re.sub('<[^<]+?>', '',  dict_ans[a_id]['body'])\n",
    "    qid = dict_ans[a_id]['parent_id']\n",
    "    ques_body = re.sub('<[^<]+?>', '',  dict_ques[qid]['body'])\n",
    "    ques_title = re.sub('<[^<]+?>', '',  dict_ques[qid]['title'])\n",
    "    dict_aid_body[a_id] = ques_title + '. ' + ques_body + '. ' + ans_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919471040e8b49589b90942c0eaa0379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_q_idx = {}\n",
    "count = 0\n",
    "for topic_id in tqdm(topic_list):\n",
    "    dict_q_idx[topic_id]=0\n",
    "    count+=1\n",
    "    \n",
    "def Sort_Tuple(tup):  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2])\n",
    "    return tup\n",
    "\n",
    "def reduce_str(body):\n",
    "#     tokenizer =  RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    encoded = tokenizer.encode(body)[:510]\n",
    "    return tokenizer.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result_list = []\n",
    "# count = 0\n",
    "# for qid in tqdm(topic_list):\n",
    "#     tup_postid_sim = []\n",
    "#     for post_id in tqdm(dict_q_a[qid]):\n",
    "#         try:\n",
    "#             token_ids = tokenizer.encode(dict_aid_body[post_id])[:256]\n",
    "#             token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "#             token_ids = token_ids.to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 output = model(token_ids)[0].squeeze()\n",
    "#             ans_emb = output[0].cpu()\n",
    "#             del output\n",
    "#             torch.cuda.empty_cache()\n",
    "            \n",
    "#         except:\n",
    "#             print('CUDA error: an illegal memory access was encountered', post_id)\n",
    "#             break\n",
    "#         result = 1 - spatial.distance.cosine(ques_emb[dict_q_idx[qid]].cpu(), ans_emb)\n",
    "        \n",
    "#         if math.isnan(result):\n",
    "#             count+=1\n",
    "#             print(qid, post_id, count)\n",
    "#             break\n",
    "#         tup_postid_sim.append((qid,post_id,result))\n",
    "#     result_list.append(Sort_Tuple(tup_postid_sim)[::-1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tf.psu-task1-cls_bert-auto-both-A.tsv', 'w') as eval_file:\n",
    "#     for res in result_list:\n",
    "#         count = 1\n",
    "#         for tuples in res:\n",
    "#             eval_file.write(tuples[0]+'\\t'+ '1\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'bert_cls'+'\\n')\n",
    "# #             eval_file.write(tuples[0]+'\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'mlt_bert'+'\\n')\n",
    "#             count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m P qrel_partial_task1 ../runs/tf.psu-task1-mlt.base-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.psu-task1-cls_bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -m ndcg qrel_partial_task1 tf.psu-task1-cls_bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecEval, fusion\n",
    "\n",
    "runs_path = '/data/szr207/projects/ArqMath/runs'\n",
    "r1 = TrecRun(os.path.join(\"tf.psu-task1-cls_bert-auto-both-A.tsv\"))\n",
    "r2 = TrecRun(os.path.join(runs_path, \"tf.psu-task1-mlt.base-auto-both-A.tsv\"))\n",
    "\n",
    "# Easy way to create new baselines by fusing existing runs:\n",
    "fused_run = fusion.reciprocal_rank_fusion([r1,r2])\n",
    "\n",
    "# Save run to disk with all its topics\n",
    "fused_run.print_subset(\"tf.psu-task1-rrf.cls.bert-auto-both-P.tsv\", topics=fused_run.topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -m ndcg qrel_partial_task1 tf.psu-task1-rrf.cls.bert-auto-both-P.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -m ndcg qrel_partial_task1 ../runs/tf.psu-task1-mlt.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "result_list = []\n",
    "count = 0\n",
    "for qid in tqdm(topic_list):\n",
    "    tup_postid_sim = []\n",
    "    for post_id in dict_q_a[qid]:\n",
    "        try:\n",
    "            token_ids = tokenizer.encode(dict_aid_body[post_id])[:SEQ_LENGTH]\n",
    "            token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "            token_ids = token_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model(input_ids=token_ids)\n",
    "            hidden_states = out[2]\n",
    "            del out\n",
    "            torch.cuda.empty_cache()\n",
    "            ans_emb = torch.mean(hidden_states[-1], dim=1).squeeze().cpu()\n",
    "        except:\n",
    "            print('CUDA error: an illegal memory access was encountered', post_id)\n",
    "            break\n",
    "        result = 1 - spatial.distance.cosine(ques_emb[dict_q_idx[qid]].cpu(), ans_emb)\n",
    "\n",
    "        tup_postid_sim.append((qid,post_id,result))\n",
    "    result_list.append(Sort_Tuple(tup_postid_sim)[::-1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf.psu-task1-prim.roberta-base-auto-both-A.tsv', 'w') as eval_file:\n",
    "    for res in result_list:\n",
    "        count = 1\n",
    "        for tuples in res:\n",
    "            eval_file.write(tuples[0]+'\\t'+ '1\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'bert_cls'+'\\n')\n",
    "#             eval_file.write(tuples[0]+'\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'mlt_bert'+'\\n')\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -m ndcg ../runs/qrel_task1 tf.psu-task1-prim.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecEval, fusion\n",
    "\n",
    "runs_path = '/data/szr207/projects/ArqMath/runs'\n",
    "r1 = TrecRun(os.path.join(\"tf.psu-task1-mean.bert-auto-both-A.tsv\"))\n",
    "r2 = TrecRun(os.path.join(\"tf.psu-task1-77.mlt-auto-both-A.tsv\"))\n",
    "\n",
    "# Easy way to create new baselines by fusing existing runs:\n",
    "fused_run = fusion.reciprocal_rank_fusion([r1,r2])\n",
    "\n",
    "# Save run to disk with all its topics\n",
    "fused_run.print_subset(\"tf.psu-task1-rrf.77..mlt.anserini-auto-both-P.tsv\", topics=fused_run.topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m P qrel_partial_task1 tf.psu-task1-rrf.mean.bert-auto-both-P.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge MLT and Anserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecEval, fusion\n",
    "\n",
    "runs_path = '/data/szr207/projects/ArqMath/runs'\n",
    "r1 = TrecRun(os.path.join(\"tf.psu-task1-77.anserini-auto-both-A.tsv\"))\n",
    "r2 = TrecRun(os.path.join(\"tf.psu-task1-77.mlt-auto-both-A.tsv\"))\n",
    "r2 = TrecRun(os.path.join(\"tf.psu-task1-77.mlt-auto-both-A.tsv\"))\n",
    "\n",
    "# Easy way to create new baselines by fusing existing runs:\n",
    "fused_run = fusion.reciprocal_rank_fusion([r1,r2])\n",
    "\n",
    "# Save run to disk with all its topics\n",
    "fused_run.print_subset(\"tf.psu-task1-rrf.77.mlt.anserini-auto-both-A.tsv\", topics=fused_run.topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-rrf.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 ../runs/tf.psu-task1-anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 ../runs/tf.psu-task1-mlt.base-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing not judged ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_q = []\n",
    "with open('../runs/qrel_task1', 'r') as eval_file:\n",
    "    for _,line in enumerate(eval_file):\n",
    "        list_q.append(line.split('\\t')[0])\n",
    "\n",
    "list_q = list(set(list_q))\n",
    "len(list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-77.mlt-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-77.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 ./77_runs/tf.psu-task1-rrf.77.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m map ../runs/qrel_task1 tf.psu-task1-rrf.77.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -m map ../runs/qrel_task1 77_runs/tf.psu-task1-rrf.77.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, procedures\n",
    "\n",
    "qrels_file = \"../runs/qrel_task1\"\n",
    "qrels = TrecQrel(qrels_file)\n",
    "\n",
    "# Generates a P@10 graph with all the runs in a directory\n",
    "path_to_runs = \"./77_runs\"\n",
    "runs = procedures.list_of_runs_from_path(path_to_runs, \"*.tsv\")\n",
    "\n",
    "results = procedures.evaluate_runs(runs, qrels, per_query=True)\n",
    "p10 = procedures.extract_metric_from_results(results, \"P_10\")\n",
    "fig = procedures.plot_system_rank(p10, display_metric=\"P@10\", outfile=\"plot.pdf\")\n",
    "fig.savefig(\"plot.pdf\", bbox_inches='tight', dpi=600)\n",
    "# Sample output with one run for each participating team in robust03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, TrecRun, TrecEval\n",
    "\n",
    "# A typical evaluation workflow\n",
    "r1 = TrecRun(\"./77_runs/tf.psu-task1-rrf.77.mlt.anserini-auto-both-A.tsv\")\n",
    "r1.topics()[:5] # Shows the first 5 topics: 601, 602, 603, 604, 605\n",
    "\n",
    "qrels = TrecQrel(\"../runs/qrel_task1\")\n",
    "\n",
    "te = TrecEval(r1, qrels)\n",
    "rbp, residuals = te.get_rbp()           # RBP: 0.474, Residuals: 0.001\n",
    "p100 = te.get_precision(depth=100)     # P@100: 0.186\n",
    "\n",
    "# Check if documents retrieved by the system were judged:\n",
    "cover10 = r1.get_mean_coverage(qrels, topX=10)   # 9.99\n",
    "cover1000 = r1.get_mean_coverage(qrels, topX=1000) # 481.390 \n",
    "# On average for system 'input.aplrob03a' participating in robust03, 480 documents out of 1000 were judged.\n",
    "print(\"Average number of documents judged among top 10: %.2f, among top 1000: %.2f\" % (cover10, cover1000))\n",
    "\n",
    "# Loads another run\n",
    "r2 = TrecRun(\"./77_runs/tf.psu-task1-77.anserini-auto-both-A.tsv\")\n",
    "\n",
    "# Check how many documents, on average, in the top 10 of r1 were retrieved in the top 10 of r2\n",
    "r1.check_run_coverage(r2, topX=10) # 3.64\n",
    "\n",
    "# Evaluates r1 and r2 using all implemented evaluation metrics\n",
    "result_r1 = r1.evaluate_run(qrels, per_query=True) \n",
    "result_r2 = r2.evaluate_run(qrels, per_query=True)\n",
    "\n",
    "# Inspect for statistically significant differences between the two runs for  P_10 using two-tailed Student t-test\n",
    "pvalue = result_r1.compare_with(result_r2, metric=\"P_10\") # pvalue: 0.0167 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.get_precision(depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.get_ndcg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-77.mean.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_q = []\n",
    "with open('../final_runs/prim_psu-task1-auto-both-A.tsv', 'r') as eval_file:\n",
    "    for _,line in enumerate(eval_file):\n",
    "        list_q.append(line.split('\\t')[0])\n",
    "\n",
    "list_q = list(set(list_q))\n",
    "len(list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 ../final_runs/psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 ../final_runs/prim_psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../final_runs/prim_psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l ../final_runs/psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../final_runs/prim_psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../runs/qrel_task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../final_runs/prim_psu-task1-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-prim.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecRun, TrecEval, fusion\n",
    "\n",
    "runs_path = '/data/szr207/projects/ArqMath/runs'\n",
    "r1 = TrecRun(os.path.join(\"tf.psu-task1-prim.mlt-auto-both-A.tsv\"))\n",
    "r2 = TrecRun(os.path.join(\"tf.psu-task1-prim.anserini-auto-both-A.tsv\"))\n",
    "r3 = TrecRun(os.path.join(\"tf.psu-task1-prim.bert-auto-both-A.tsv\"))\n",
    "# Easy way to create new baselines by fusing existing runs:\n",
    "fused_run = fusion.reciprocal_rank_fusion([r1,r2,r3])\n",
    "\n",
    "# Save run to disk with all its topics\n",
    "fused_run.print_subset(\"tf.psu-task1-rrf.prim.mlt.anserini.bert-auto-both-A.tsv\", topics=fused_run.topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-prim.mlt-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-rrf.prim.mlt.anserini.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l tf.psu-task1-prim.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-rrf.prim.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-rrf.prim.mlt.anserini.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar jtreceval-0.0.5-jar-with-dependencies.jar -l2 -m ndcg ../runs/qrel_task1 tf.psu-task1-prim.anserini.bert-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.psu-task1-rrf.prim.mlt.anserini.bert-auto-both-A.tsv and tf.psu-task1-rrf.prim.mlt.anserini-auto-both-A.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trectools import TrecQrel, procedures\n",
    "\n",
    "qrels_file = \"../runs/qrel_task1\"\n",
    "qrels = TrecQrel(qrels_file)\n",
    "\n",
    "# Generates a P@10 graph with all the runs in a directory\n",
    "path_to_runs = \"./77_runs\"\n",
    "runs = procedures.list_of_runs_from_path(path_to_runs, \"*.tsv\")\n",
    "\n",
    "results = procedures.evaluate_runs(runs, qrels, per_query=True)\n",
    "p10 = procedures.extract_metric_from_results(results, \"map\")\n",
    "fig = procedures.plot_system_rank(p10, display_metric=\"map\", outfile=\"plot.pdf\")\n",
    "fig.savefig(\"plot.pdf\", bbox_inches='tight', dpi=600)\n",
    "# Sample output with one run for each participating team in robust03:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
