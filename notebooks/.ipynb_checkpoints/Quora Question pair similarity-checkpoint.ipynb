{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;font-size:30px;\" > Quora Question Pairs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1. Business Problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.1 Description </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.</p>\n",
    "<p>\n",
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "</p>\n",
    "<br>\n",
    "> Credits: Kaggle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Problem Statement __\n",
    "- Identify which questions asked on Quora are duplicates of questions that have already been asked. \n",
    "- This could be useful to instantly provide answers to questions that have already been answered. \n",
    "- We are tasked with predicting whether a pair of questions are duplicates or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.2 Sources/Useful Links</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source : https://www.kaggle.com/c/quora-question-pairs\n",
    "<br><br>____ Useful Links ____\n",
    "- Discussions : https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments\n",
    "- Kaggle Winning Solution and other approaches: https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\n",
    "- Blog 1 : https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning\n",
    "- Blog 2 : https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3 Real world/Business Objectives and Constraints </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The cost of a mis-classification can be very high.\n",
    "2. You would want a probability of a pair of questions to be duplicates so that you can choose any threshold of choice.\n",
    "3. No strict latency concerns.\n",
    "4. Interpretability is partially important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2. Machine Learning Probelm </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.1 Data </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1.1 Data Overview </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "- Data will be in a file Train.csv <br>\n",
    "- Train.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate <br>\n",
    "- Size of Train.csv - 60MB <br>\n",
    "- Number of rows in Train.csv = 404,290\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.1.2 Example Data point </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\"id\",\"qid1\",\"qid2\",\"question1\",\"question2\",\"is_duplicate\"\n",
    "\"0\",\"1\",\"2\",\"What is the step by step guide to invest in share market in india?\",\"What is the step by step guide to invest in share market?\",\"0\"\n",
    "\"1\",\"3\",\"4\",\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\",\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\",\"0\"\n",
    "\"7\",\"15\",\"16\",\"How can I be a good geologist?\",\"What should I do to be a great geologist?\",\"1\"\n",
    "\"11\",\"23\",\"24\",\"How do I read and find my YouTube comments?\",\"How can I see all my Youtube comments?\",\"1\"\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.2 Mapping the real world problem to an ML problem </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2.1 Type of Machine Leaning Problem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> It is a binary classification problem, for a given pair of questions we need to predict if they are duplicate or not. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 2.2.2 Performance Metric </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/c/quora-question-pairs#evaluation\n",
    "\n",
    "Metric(s): \n",
    "* log-loss : https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "* Binary Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2.3 Train and Test Construction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>  </p>\n",
    "<p> We build train and test by randomly splitting in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3. Exploratory Data Analysis </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from subprocess import check_output\n",
    "%matplotlib inline\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# This package is used for finding longest common subsequence between two strings\n",
    "# you can write your own dp code for this\n",
    "import distance\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "# Import the Required lib packages for WORD-Cloud generation\n",
    "# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from os import path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.1 Reading data and basic stats </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 3822268\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/szr207/projects/ArqMath/gold/train.tsv\",sep='\\t')\n",
    "\n",
    "print(\"Number of data points:\",df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16020</td>\n",
       "      <td>16025</td>\n",
       "      <td>Equivalent statements of the Axiom of Choice. ...</td>\n",
       "      <td>Assume the first form. Let I be any set and le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16020</td>\n",
       "      <td>697264</td>\n",
       "      <td>Equivalent statements of the Axiom of Choice. ...</td>\n",
       "      <td>I know two equivalent definitions for an onto ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>16020</td>\n",
       "      <td>2645743</td>\n",
       "      <td>Equivalent statements of the Axiom of Choice. ...</td>\n",
       "      <td>This is part of the all-important Corresponden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16020</td>\n",
       "      <td>2875568</td>\n",
       "      <td>Equivalent statements of the Axiom of Choice. ...</td>\n",
       "      <td>I am going to answer my own question. I had a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>16020</td>\n",
       "      <td>1650515</td>\n",
       "      <td>Equivalent statements of the Axiom of Choice. ...</td>\n",
       "      <td>More informally, elements of the free semigrou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   qid1     qid2                                          question1  \\\n",
       "0   1  16020    16025  Equivalent statements of the Axiom of Choice. ...   \n",
       "1   2  16020   697264  Equivalent statements of the Axiom of Choice. ...   \n",
       "2   3  16020  2645743  Equivalent statements of the Axiom of Choice. ...   \n",
       "3   4  16020  2875568  Equivalent statements of the Axiom of Choice. ...   \n",
       "4   5  16020  1650515  Equivalent statements of the Axiom of Choice. ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Assume the first form. Let I be any set and le...             0  \n",
       "1  I know two equivalent definitions for an onto ...             0  \n",
       "2  This is part of the all-important Corresponden...             0  \n",
       "3  I am going to answer my own question. I had a ...             0  \n",
       "4  More informally, elements of the free semigrou...             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3822268 entries, 0 to 3822267\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   id            int64 \n",
      " 1   qid1          int64 \n",
      " 2   qid2          int64 \n",
      " 3   question1     object\n",
      " 4   question2     object\n",
      " 5   is_duplicate  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 175.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a minimal number of data fields here, consisting of:\n",
    "\n",
    "- id:  Looks like a simple rowID\n",
    "- qid{1, 2}:  The unique ID of each question in the pair\n",
    "- question{1, 2}:  The actual textual contents of the questions.\n",
    "- is_duplicate:  The label that we are trying to predict - whether the two questions are duplicates of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2.1 Distribution of data points among output classes</h3>\n",
    "- Number of duplicate(smilar) and non-duplicate(non similar) questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f16bbbf3dd8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby(\"is_duplicate\")['id'].count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~> Total number of question pairs for training:\n",
      "   3822268\n"
     ]
    }
   ],
   "source": [
    "print('~> Total number of question pairs for training:\\n   {}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~> Question pairs are not Similar (is_duplicate = 0):\n",
      "   90.91%\n",
      "\n",
      "~> Question pairs are Similar (is_duplicate = 1):\n",
      "   9.09%\n"
     ]
    }
   ],
   "source": [
    "print('~> Question pairs are not Similar (is_duplicate = 0):\\n   {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\n",
    "print('\\n~> Question pairs are Similar (is_duplicate = 1):\\n   {}%'.format(round(df['is_duplicate'].mean()*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2.2 Number of unique questions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of  Unique Questions are: 1257256\n",
      "\n",
      "Number of unique questions that appear more than one time: 914862 (72.76656464554554%)\n",
      "\n",
      "Max number of times a single question is repeated: 378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\n",
    "unique_qs = len(np.unique(qids))\n",
    "qs_morethan_onetime = np.sum(qids.value_counts() > 1)\n",
    "print ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n",
    "#print len(np.unique(qids))\n",
    "\n",
    "print ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\n",
    "\n",
    "print ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n",
    "\n",
    "q_vals=qids.value_counts()\n",
    "\n",
    "q_vals=q_vals.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF2CAYAAABKyB3wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3debwkVX338c8XBkRZDTMmwgCDOIq4y7gkbmM0CkbBxCXggriNJooxERWXRxDFxy1PHo0gEhdEBEGjhigGjYooCjIoO2ImLDKgMiCgPLihv+ePqgs1zb1zeziXuXfg8369+nW7q06dOlVdXf3tU+d2p6qQJEnSbbPBbDdAkiRpfWaYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCY0pyU5OQkL53tdqxPkrwpyUdmux2jknw5yQtnux0tkhyU5OjZbsftLcnSJCtnux1rkuSxSS6a7XZIQ4YpzZoklyb5VZIbkvwsyZFJNlvLOhYlqSTzbq92zkWTvelV1Turas4F0Kravao+MdvtuDPoX0PvmO12zKT+9X3vicdV9a2quu9stkkaZZjSbHt6VW0GPAxYArzl9l7hTAavdHwd6WZ3tmAvyTClOaKqrgC+DDxgdF6SDZK8JcllSa5KclSSLfvZp/R/r+t7uP50kuUPSvLZJEcn+QWwb5Itk3w0yU+SXJHkHUk27Mvvm+TUJB9Mcn2SHyZ54qC+k5MckuRU4EbgXkl2TvLVJD9PclGS5wzKPzXJBUl+2a9r/8G8pyU5K8l1Sb6T5EGDeZcm2T/JOX07jkuySZJN+321Tb/NNyTZZngpatBj98IkP05ydZI3D+q+a5JPJLk2yYVJXj/V5Z3Jev+Gl2H7/fXtJO/r67skye5TlN2wL3d1kouTvHJYd7/NTxp57o4ePH5Uv5+uS3J2kqWTtbkve0CS/+n3+wVJ/mowb7o275jkm/2yXwXmr2E9S5OsTPKGJD8FPt4fsxPrvybJ8Un+aGR/LktyZX8MDo+JKZft538myU/7Y+KUJPfvpy8Dnge8vj8m/qOfvk2Sf0uyqt/OVw/qumu63qxrk1wAPHyq7ezL/0W618P16V4f3xw8t6PP1WrHTdb8mrt3X9f1/bFxXD994vV9dr9Nf5ORXtkk9+uPseuSnJ9kj8G8I5McmuRL/XN5epKd+nlJ8s/pzim/SHJukludf6RxGKY0JyTZDngq8INJZu/b354A3AvYDPhgP+9x/d+tqmqzqvruFKvYE/gssBXwKeBI4Cbg3sBDgScDw0tkjwT+h+5N9EDgc8M3NOAFwDJgc2AV8FXgGOAewF7AYUl26ct+FHh5VW1OFxa/3m/zQ4GPAS8HtgY+DJyQ5C6D9TwH2A3YEXgQsG9V/T9gd+DKfps3q6orp9juxwD3BZ4IvDXJ/frpBwKL6PbnXwDPn2L5cT0SuIhuf70H+GiSTFLuZcDT6Pb5EuBZ464gybbAl4B3AH8E7A/8W5IFUyzyP8BjgS2BtwFHJ7nnmG0+Bjizn/d2YLoxX3/St2kHuuNiP+AZwOOBbYBrgUNHlnkCsJju2HvDIEROt+yX++XuAXyf7nimqo7o77+nPyaenq7X9D+As4Ft6Y6D1yR5Sl/XgcBO/e0pa9rOJPOBz9H1Hs+n27+Pnma/DB3J1K+5twNfAe4OLAT+pd+midf3g/ttOm6kTRv12/eVfn/sB3wqyfAy4F50z//dgRXAIf30J9OdP+5Dd4w8B7hmLbZHukVVzdqN7o3kKuC8Mcs/B7gAOB84Zjbb7m1Gnv9LgRuA64DLgMOAu/bzTgZe2t//GvB3g+XuC/wOmEcXCAqYt4b1HAScMnj8x8BvJtbVT9sb+EZ/f1/gSiCD+d8DXjBo28GDeX8DfGtknR8GDuzv/5guMG0xUuZDwNtHpl0EPH6wf54/mPce4PD+/lJg5STbeXR/f2K/LBzZhr36+xcDTxnMe+lofYN5t9rHI8/PvsCKwby79eX/ZJKyXwdeMSj75GHd/TY/aYptegPwyZG2nQS8cMzj7Sxgz+naDGxP96a/6WD+MRPtmKTepcBvgU0G0y4Enjh4fE9ufczuPPLcfnS6ZSdZ91Z9XVv2j48E3jGY/0jgxyPLvBH4+OA42G0wb9kajoN9gNMGjwOsHDy3Nz9Xo8cN07/mjgKOYHC8DsoVcO+R/b2yv/9Y4KfABoP5xwIHDfbHRwbzngr8sL//58CPgEcNl/fm7bbcZrtn6ki6T93TSrKY7iTw6Kq6P/Ca27FdWneeUVVbVdUOVfV3VfWrScpsQxe2JlzGLSfocV0+uL8DsBHwk/7SwHV04ecegzJXVNXwV8Av69sxVX2PnKirr+95dG/MAM+kO4lf1l/K+NPBcq8dWW67kfX8dHD/RrpeubUx1fLbjGzD8P5tcfN6qurG/u5kbR1d72WTlJnKDsCzR/bXY+jCxq0k2Se3XEK9jq5XcHi5bqo2bwNcW10P4LjtXFVVvx5p6+cH674Q+D2rH7Oj+2Gb6ZZNd5n0Xf0lwF/QhU+Y+jLkDnSXg4f77E2DdqzN87Fa2f71Me5xM91r7vV04ex7/aW6F49Z7zbA5VX1h5Ft2HbweNLXQFV9na6H+1DgqiRHJNlizPVKq5nVMFVVpwA/H05LslOS/0xyZpJvJdm5n/Uy4NCqurZf9qp13FzNnivpTsYTJnoOfkb3qXUcw3KX031Knt8Hua2qaos+pE/YduQy1fZ9O6aq75uDuiYuOf4tQFWdUVV70r1xfAE4frDcISPL3a2qjl3L7bktfkJ3OWXCdmsoOxEq7jaY9ieTFRxzvcN1bT/JuqZaz+V0PVPD/bVpVb1rdCVJdgD+FXgVsHVVbQWcR/eGPU4b755ubNpU7Rw1+nxcDuw+0tZNqhsbOGF0P1w5xrLPpbtk/SS6S1OLJjZ5De24ZKSuzavqqYNtXdPzMbRa2f71MVx2uuduytdcVf20ql5WVdvQ9eIelsF/8K3BlcB2Wf2fQLYHrpii/Gqq6gNVtSuwC93lvteNs5w0arZ7piZzBLBff4DvT3fpB7oD/T7pBgaflmSsHi3dIRwL/EO6QcGbAe8Ejquqm+jGK/2BbuzPWKrqJ3RjLP4pyRbpBvzulOTxg2L3AF6dZKMkzwbuB5w4RZVfpDs2X9CX3yjJw/uBsRsneV6SLavqd8Av+vZC92b/iiSP7AfDbprkL5NsPsZm/AzYOrcMxF9bxwNvTHL3fizSq6YqWFWr6N6cnt/3jLyYbozNbV3vq5MsTHJ34ICR+WcBe/X7cHRM1dHA05M8pW/HJukGIy/k1jalCxarAJK8iEn+uWEyVXUZsBx4W//8PQZ4+tpsJHA4cEgf6kiyIMmeI2X+V5K7pRtA/iLguDGW3ZwulFxDF1zeOVLnz1j9tfA94JfpBsfftd9vD0gyMdB8eBwspBtzNJUvAfdP8tfpBpW/mtUD01nA45Js3x+Xb5yYMd1rLsmzB8/jtXTP3cTrZHSbhk6n6216fX/MLKV7rj69hu2gX+fD+9feRnRB8NeDdUprZU6Fqf6N8s+AzyQ5i64beKILfx7doMuldNfa/zXJVrPRTq1zHwM+Sfefe5fQnfT2g5svzxwCnNpfPnjUmHXuA2xMNwbvWrrB6cPLRafTHW9X9/U/q6omHZxaVb+kG/uzF90n5Z8C7wYmBpK/ALi0vyzzCrpLgFTVcroe1w/2bVhBN5ZnWlX1Q7qQeXG/3dtMt8yIg+nGu1wC/Bfd9v9mDeVfRvep/Rrg/sB31nJ9E/6VbpzT2XSDpz83Mv9/0QW1a+kGDR8zMaOqLqfrlXkTXUi6vG/Trc5jVXUB8E/Ad+nejB8InLoW7Xwu3Xijn9MN0j5qLZYFeD9wAvCVJL8ETuvrG/om3XP+NeB9VfWVMZY9iu4y1hV0x+5pI3V+FNilPya+UFW/pxvw/xC65/pq4CN0vVrQ7ePL+nlfoXudTaqqrgaeDbyL7jhYzGCfVtVX6QLhOXSD9784UsWaXnMPB05PckO/7X9fVRf38w4CPtFv03OGFVbVb+nC0+79th0G7NO/PqazBd3xeG2/D64B3jvGctKtZPVhIbPQgGQR8MWqekB/vfqiqrrVGIgkhwOnV9XH+8dfAw6oqjPWZXt1x5dkX7pBtY+Z7basK0n+lm5w+uOnLTyz611E90a+Ud/TeId3R9rmJCfTDTqfc9+8L61Lc6pnqqp+AVzSX1aZ+B6QB/ezv0DXKzXxL7r3oftPFElrKck9kzy6v9xyX+C1wOdnu12StD6a1TCV5Fi6bvj7pvvSu5fQXQJ5SZKz6b4CYWKswEnANem+WO4bwOumuuwiaVob011G/yXd1xX8O7eMT5QkrYVZv8wnSZK0PptTl/kkSZLWN4YpSZKkBrP26+bz58+vRYsWzdbqJUmSxnbmmWdeXVWT/hborIWpRYsWsXz58tlavSRJ0tiSTPlzS17mkyRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJajBvthuwruz6uqNmuwnSndKZ791ntpsgSbcre6YkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaTBumknwsyVVJzpti/vOSnJPk3CTfSfLgmW+mJEnS3DROz9SRwG5rmH8J8PiqeiDwduCIGWiXJEnSemHaHzquqlOSLFrD/O8MHp4GLGxvliRJ0vphpsdMvQT48lQzkyxLsjzJ8lWrVs3wqiVJkta9GQtTSZ5AF6beMFWZqjqiqpZU1ZIFCxbM1KolSZJmzbSX+caR5EHAR4Ddq+qamahTkiRpfdDcM5Vke+BzwAuq6kftTZIkSVp/TNszleRYYCkwP8lK4EBgI4CqOhx4K7A1cFgSgJuqasnt1WBJkqS5ZJz/5tt7mvkvBV46Yy2SJElaj/gN6JIkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ0MU5IkSQ2mDVNJPpbkqiTnTTE/ST6QZEWSc5I8bOabKUmSNDeN0zN1JLDbGubvDizub8uAD7U3S5Ikaf0wbZiqqlOAn6+hyJ7AUdU5DdgqyT1nqoGSJElz2UyMmdoWuHzweGU/TZIk6Q5vnQ5AT7IsyfIky1etWrUuVy1JknS7mIkwdQWw3eDxwn7arVTVEVW1pKqWLFiwYAZWLUmSNLtmIkydAOzT/1ffo4Drq+onM1CvJEnSnDdvugJJjgWWAvOTrAQOBDYCqKrDgROBpwIrgBuBF91ejZUkSZprpg1TVbX3NPMLeOWMtUiSJGk94jegS5IkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNTBMSZIkNZg32w2QpPXZjw9+4Gw3QbpT2v6t5852E25mz5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVIDw5QkSVKDscJUkt2SXJRkRZIDJpm/fZJvJPlBknOSPHXmmypJkjT3TBumkmwIHArsDuwC7J1kl5FibwGOr6qHAnsBh810QyVJkuaicXqmHgGsqKqLq+q3wKeBPUfKFLBFf39L4MqZa6IkSdLcNU6Y2ha4fPB4ZT9t6CDg+UlWAicC+01WUZJlSZYnWb5q1arb0FxJkqS5ZaYGoO8NHFlVC4GnAp9Mcqu6q+qIqlpSVUsWLFgwQ6uWJEmaPeOEqSuA7QaPF/bThl4CHA9QVd8FNgHmz0QDJUmS5rJxwtQZwOIkOybZmG6A+QkjZX4MPBEgyf3owpTX8SRJ0h3etGGqqm4CXgWcBFxI91975yc5OMkefbHXAi9LcjZwLLBvVdXt1WhJkqS5Yt44harqRLqB5cNpbx3cvwB49Mw2TZIkae7zG9AlSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIaGKYkSZIajBWmkuyW5KIkK5IcMEWZ5yS5IMn5SY6Z2WZKkiTNTfOmK5BkQ+BQ4C+AlcAZSU6oqgsGZRYDbwQeXVXXJrnH7dVgSZKkuWScnqlHACuq6uKq+i3waWDPkTIvAw6tqmsBquqqmW2mJEnS3DROmNoWuHzweGU/beg+wH2SnJrktCS7zVQDJUmS5rJpL/OtRT2LgaXAQuCUJA+squuGhZIsA5YBbL/99jO0akmSpNkzTs/UFcB2g8cL+2lDK4ETqup3VXUJ8CO6cLWaqjqiqpZU1ZIFCxbc1jZLkiTNGeOEqTOAxUl2TLIxsBdwwkiZL9D1SpFkPt1lv4tnsJ2SJElz0rRhqqpuAl4FnARcCBxfVecnOTjJHn2xk4BrklwAfAN4XVVdc3s1WpIkaa4Ya8xUVZ0InDgy7a2D+wX8Y3+TJEm60/Ab0CVJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhoYpiRJkhqMFaaS7JbkoiQrkhywhnLPTFJJlsxcEyVJkuauacNUkg2BQ4HdgV2AvZPsMkm5zYG/B06f6UZKkiTNVeP0TD0CWFFVF1fVb4FPA3tOUu7twLuBX89g+yRJkua0ccLUtsDlg8cr+2k3S/IwYLuq+tKaKkqyLMnyJMtXrVq11o2VJEmaa5oHoCfZAPg/wGunK1tVR1TVkqpasmDBgtZVS5IkzbpxwtQVwHaDxwv7aRM2Bx4AnJzkUuBRwAkOQpckSXcG44SpM4DFSXZMsjGwF3DCxMyqur6q5lfVoqpaBJwG7FFVy2+XFkuSJM0h04apqroJeBVwEnAhcHxVnZ/k4CR73N4NlCRJmsvmjVOoqk4EThyZ9tYpyi5tb5YkSdL6wW9AlyRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJamCYkiRJajBWmEqyW5KLkqxIcsAk8/8xyQVJzknytSQ7zHxTJUmS5p5pw1SSDYFDgd2BXYC9k+wyUuwHwJKqehDwWeA9M91QSZKkuWicnqlHACuq6uKq+i3waWDPYYGq+kZV3dg/PA1YOLPNlCRJmpvGCVPbApcPHq/sp03lJcCXJ5uRZFmS5UmWr1q1avxWSpIkzVEzOgA9yfOBJcB7J5tfVUdU1ZKqWrJgwYKZXLUkSdKsmDdGmSuA7QaPF/bTVpPkScCbgcdX1W9mpnmSJElz2zg9U2cAi5PsmGRjYC/ghGGBJA8FPgzsUVVXzXwzJUmS5qZpw1RV3QS8CjgJuBA4vqrOT3Jwkj36Yu8FNgM+k+SsJCdMUZ0kSdIdyjiX+aiqE4ETR6a9dXD/STPcLkmSpPWC34AuSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUwDAlSZLUYKwwlWS3JBclWZHkgEnm3yXJcf3805MsmumGSpIkzUXThqkkGwKHArsDuwB7J9llpNhLgGur6t7APwPvnumGSpIkzUXj9Ew9AlhRVRdX1W+BTwN7jpTZE/hEf/+zwBOTZOaaKUmSNDeNE6a2BS4fPF7ZT5u0TFXdBFwPbD0TDZQkSZrL5q3LlSVZBizrH96Q5KJ1uX6t1+YDV892I7T28r4XznYTpDXx3LK+OnCdXwDbYaoZ44SpK4DtBo8X9tMmK7MyyTxgS+Ca0Yqq6gjgiDHWKa0myfKqWjLb7ZB0x+K5RTNhnMt8ZwCLk+yYZGNgL+CEkTInABMfP58FfL2qauaaKUmSNDdN2zNVVTcleRVwErAh8LGqOj/JwcDyqjoB+CjwySQrgJ/TBS5JkqQ7vNiBpPVBkmX9ZWJJmjGeWzQTDFOSJEkN/DkZSZKkBoYpSdJtkuT3Sc5Kcl6S/0iy1Tpa71ZJ/u42LHdQkv2nmLcsyQ/72/IkS5sbunr9q7U5yTZJPjuT69DsMUxpbEmWJPnAbLdjJiRZmuTPBo9fkWSf2WyTtB76VVU9pKoeQPfPR69cR+vdCljrMDWVJE8DXg48pqp2pvs+xKOTjH5BdYvV2lxVV1bVs2awfs0iw5TGVlXLq+rVs92OGbIUuDlMVdXhVXXU7DVHWu99l/7XMZLslOQ/k5yZ5FtJdu6nH5nk8L7n50d9iCHJhknem+SMJOckeXk/fbMkX0vy/STnJpn4KbN3ATv1vWLv7cu+brD82yYaleTN/bq+Ddx3ira/AXhdVV0NUFXfBz5OHw6TXJpkfn9/SZKT+/ubJvlYku8l+cFE+5Lcv592Vt+exaNtTrIoyXl9+U2SfLzfxh8keUI/fd8kn+v35X8nec9gfx3Z9wiem+QfGp87taoqb3fSG7AIOG/weH/gIOBkuh+r/h7wI+Cx/fylwBf7+1sDXwHOBz4CXEb3TcKT1tnf3wn4T+BM4FvAzmto2450J+dzgXcAN4y2oX/8QWDf/v6uwDf7+k8C7tlPfzVwAXAO3W9LLgJ+Svdls2cBj+23e/++/EOA0/rynwfu3k+far/cv592Vr/M4tl+br15Wxe3wetyQ+AzwG79469NvA6AR9J99yDAkf05YANgMd3Pk21C1xP0lr7MXYDl/TlgHrBFP30+sALIJOeZJ9N9IXT6ur8IPK4/J5wL3A3Yol9+/0m24+fAliPT9gS+0N+/FJjf318CnNzffyfw/P7+Vv15YVPgX4Dn9dM3Bu46SZtvfgy8lu5rhwB2Bn7c75d9gYvpvgh7E7rz7Hb9dn11UNdWs30s3Nlv9kxpKvOq6hHAa4ADJ5l/IPDtqro/XeDYfow6jwD2q6pd6ULWYWso+37gQ1X1QOAn01WcZCO6E9iz+vo/BhzSzz4AeGhVPQh4RVVdChwO/HN1lyi+NVLdUcAb+vLnsvr2T7ZfXgG8v6oeQneiXTlde6U7iLsmOYvuw8kfA19Nshldr+9n+nkfBu45WOb4qvpDVf03XVDYmS4M7dOXP53uw9piunD0ziTnAP9F1/P1x5O048n97QfA9/s6F9N9UPp8Vd1YVb/g1l843erJwAF9u0+mCzzb030QfFOSNwA7VNWvpqnnMcDRAFX1Q7rQdJ9+3teq6vqq+jXdh8Id6PbbvZL8S5LdgF/M7GZpba3T3+bTeuVz/d8z6T5BjXoc8NcAVfWlJNeuqbKRE+zE5LusYZFHA8/s73+SrkdoTe4LPIDuZA7dJ+WJEHYO8KkkXwC+ME07t6T7lPfNftIn6D5xT5hsv3wXeHOShcDn+jcJ6c7gV1X1kCR3o+sNfiVd79N1/YeLyYx+H0/Rhab9quqk4Ywk+wILgF2r6ndJLqULLKMC/O+q+vDI8q8ZczsuoOvt+fpg2q50PWQAN3HLsJjh+gM8s6pGf2f2wiSnA38JnNhftrx4zLaM+s3g/u/pPtBdm+TBwFPoPsw9B3jxbaxfM8CeqTu34QkCVj9JTLyAf8/ahe6p6tyA/gQ7uN1vmrom+xK0qeoPcP6g7gdW1ZP7eX8JHAo8DDij//3I2+pW+6WqjgH2AH5Fd+L884b6pfVOVd1Idzn9tcCNwCVJng2QzoMHxZ+dZIMkOwH3Ai6iC2J/2/cwk+Q+STalu7x1VR+knsAtPzT7S2DzQZ0nAS/uP7SRZNsk9wBOAZ6R5K5JNlxOPREAAAKQSURBVAeePsUmvAd4d5Kt++UfAvwVXa8adJf5du3vP3Ow3EnAfuk/wSV5aP/3XsDFVfUB4N+BB03S5qFvAc+b2Ha63q3RgHazfvzWBlX1b8Bb6M5tmkWGqTu3nwH3SLJ1krsAT1uLZU8BnguQZHfg7muqs+9iX9MJdtSp3PKzRM8bTL8M2CXJXdL9G/YT++kXAQuS/Glf/0b9INANgO2q6ht0g0y3BDZjihNbVV0PXJvksf2kF9CNw5rSFCdO6U6lqn5A1wu8N91r9iVJzqYbV7nnoOiP6cYYfpnusvuv6cZdXgB8vx+U/WG6DyufApYkORfYB/hhv65rgFP7AdjvraqvAMcA3+3LfhbYvLqB5McBZ/frO2OKtk/8LNqp6X4W7dvAM6pqVV/kbcD7kyyn+yA14e3ARsA5Sc7vH0PXU3Ref/nvAcBRo20eacJhwAZ924+jGwf6G6a2LXByX//RwBvXUFbrgN+AfieX5NXA39MNxr6Y7hPYUrpBmsv7T0DLq2pRuu9d2b+qntZ/gjuW7kX9HbqxA7tW1dWT1VlVByXZEfgQ3fiJjYBPV9XBU7RrR7qT42Z0AeU1VTXxqfM9dJ8aLwFuAE6oqiP7T5MfoAtM84D/S3fJ4Rv9tABHV9W7+k9/nwX+AOxHF8puqKr39fUcTjdo9WLgRX23+slT7JcD6ELX7+jGjjy3qn5+W54P6Y4syZF0/0AyZ79fqe+5/jhdZ8PzyzdJjcEwpRnRj2VYUv2/Ft8O9d8wEaYkrZ/WhzAl3RYOQJckrRNVte9st0G6PdgzpVmV5M3As0cmf6aqDpmsvCRJc41hSpIkqYH/zSdJktTAMCVJktTAMCVJktTAMCVJktTAMCVJktTg/wO/JD0yn9pdVQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [\"unique_questions\" , \"Repeated Questions\"]\n",
    "y =  [unique_qs , qs_morethan_onetime]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title (\"Plot representing unique and repeated questions  \")\n",
    "sns.barplot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.2.3 Checking for Duplicates </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions 0\n"
     ]
    }
   ],
   "source": [
    "#checking whether there are any repeated pair of questions\n",
    "\n",
    "pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n",
    "\n",
    "print (\"Number of duplicate questions\",(pair_duplicates).shape[0] - df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2.4 Number of occurrences of each question </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of times a single question is repeated: 378\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.hist(qids.value_counts(), bins=160)\n",
    "\n",
    "plt.yscale('log', nonposy='clip')\n",
    "\n",
    "plt.title('Log-Histogram of question appearance counts')\n",
    "\n",
    "plt.xlabel('Number of occurences of question')\n",
    "\n",
    "plt.ylabel('Number of questions')\n",
    "\n",
    "print ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.2.5 Checking for NULL values </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, qid1, qid2, question1, question2, is_duplicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Checking whether there are any rows with null values\n",
    "nan_rows = df[df.isnull().any(1)]\n",
    "print (nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two rows with null values in question2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, qid1, qid2, question1, question2, is_duplicate]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Filling the null values with ' '\n",
    "df = df.fillna('')\n",
    "nan_rows = df[df.isnull().any(1)]\n",
    "print (nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.3 Basic Feature Extraction (before cleaning) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now construct a few features like:\n",
    " - ____freq_qid1____ = Frequency of qid1's\n",
    " - ____freq_qid2____ = Frequency of qid2's \n",
    " - ____q1len____ = Length of q1\n",
    " - ____q2len____ = Length of q2\n",
    " - ____q1_n_words____ = Number of words in Question 1\n",
    " - ____q2_n_words____ = Number of words in Question 2\n",
    " - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n",
    " - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n",
    " - ____word_share____ = (word_common)/(word_Total)\n",
    " - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n",
    " - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    df = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
    "    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
    "    df['q1len'] = df['question1'].str.len() \n",
    "    df['q2len'] = df['question2'].str.len()\n",
    "    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "\n",
    "    def normalized_word_Common(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
    "\n",
    "    def normalized_word_Total(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
    "\n",
    "    def normalized_word_share(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
    "\n",
    "    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
    "    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
    "\n",
    "    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3.3.1 Analysis of some of the extracted features </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are some questions have only one single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Minimum length of the questions in question1 : \" , min(df['q1_n_words']))\n",
    "\n",
    "print (\"Minimum length of the questions in question2 : \" , min(df['q2_n_words']))\n",
    "\n",
    "print (\"Number of Questions with minimum length [question1] :\", df[df['q1_n_words']== 1].shape[0])\n",
    "print (\"Number of Questions with minimum length [question2] :\", df[df['q2_n_words']== 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.3.1.1 Feature: word_share </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity\n",
    "- The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.3.1.2 Feature: word_Common </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'word_Common', data = df[0:])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> The distributions of the word_Common feature in similar and non-similar questions are highly overlapping </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/12468179/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    df = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "    df.head()\n",
    "else:\n",
    "    print(\"get df_fe_without_preprocessing_train.csv from drive or run the previous notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.4 Preprocessing of Text </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing:\n",
    "    - Removing html tags \n",
    "    - Removing Punctuations\n",
    "    - Performing stemming\n",
    "    - Removing Stopwords\n",
    "    - Expanding contractions etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the results in 4 decemal points\n",
    "SAFE_DIV = 0.0001 \n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "    \n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "               \n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function to Compute and get the features : With 2 parameters of Question 1 and Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.5 Advanced Feature Extraction (NLP and Fuzzy Features) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "- __Token__: You get a token by splitting sentence a space\n",
    "- __Stop_Word__ : stop words as per NLTK.\n",
    "- __Word__ : A token that is not a stop_word\n",
    "\n",
    "\n",
    "Features:\n",
    "- __cwc_min__ :  Ratio of common_word_count to min lenghth of word count of Q1 and Q2 <br>cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n",
    "<br>\n",
    "<br>\n",
    "- __cwc_max__ :  Ratio of common_word_count to max lenghth of word count of Q1 and Q2 <br>cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n",
    "<br>\n",
    "<br>\n",
    "- __csc_min__ :  Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2 <br> csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n",
    "<br>\n",
    "<br>\n",
    "- __csc_max__ :  Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2<br>csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n",
    "<br>\n",
    "<br>\n",
    "- __ctc_min__ :  Ratio of common_token_count to min lenghth of token count of Q1 and Q2<br>ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __ctc_max__ :  Ratio of common_token_count to max lenghth of token count of Q1 and Q2<br>ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n",
    "<br>\n",
    "<br>\n",
    "        \n",
    "- __last_word_eq__ :  Check if Last word of both questions is equal or not<br>last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __first_word_eq__ :  Check if First word of both questions is equal or not<br>first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n",
    "<br>\n",
    "<br>\n",
    "        \n",
    "- __abs_len_diff__ :  Abs. length difference<br>abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __mean_len__ :  Average Token Length of both Questions<br>mean_len = (len(q1_tokens) + len(q2_tokens))/2\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- __fuzz_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- __fuzz_partial_ratio__ :  https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- __token_sort_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- __token_set_ratio__ : https://github.com/seatgeek/fuzzywuzzy#usage\n",
    "http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- __longest_substr_ratio__ :  Ratio of length longest common substring to min lenghth of token count of Q1 and Q2<br>longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "# get the Longest Common sub string\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "\n",
    "def extract_features(df):\n",
    "    # preprocessing each question\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    print(\"token features...\")\n",
    "    \n",
    "    # Merging Features with dataset\n",
    "    \n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "   \n",
    "    #Computing Fuzzy Features and Merging with Dataset\n",
    "    \n",
    "    # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n",
    "    # https://github.com/seatgeek/fuzzywuzzy\n",
    "    print(\"fuzzy features..\")\n",
    "\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n",
    "    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "    df.fillna('')\n",
    "else:\n",
    "    print(\"Extracting features for train:\")\n",
    "    df = pd.read_csv(\"/data/szr207/projects/ArqMath/gold/train.tsv\",sep='\\t')\n",
    "    df = extract_features(df)\n",
    "    df.to_csv(\"nlp_features_train.csv\", index=False)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.5.1 Analysis of extracted features </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Word Cloud of Duplicates and Non-Duplicates Question pairs\n",
    "- We can observe the most frequent occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duplicate = df[df['is_duplicate'] == 1]\n",
    "dfp_nonduplicate = df[df['is_duplicate'] == 0]\n",
    "\n",
    "# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\n",
    "p = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\n",
    "n = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n",
    "\n",
    "print (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\n",
    "print (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n",
    "\n",
    "#Saving the np array into a text file\n",
    "np.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\n",
    "np.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the text files and removing the Stop Words:\n",
    "d = path.dirname('.')\n",
    "\n",
    "textp_w = open(path.join(d, 'train_p.txt')).read()\n",
    "textn_w = open(path.join(d, 'train_n.txt')).read()\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"br\")\n",
    "stopwords.add(\" \")\n",
    "stopwords.remove(\"not\")\n",
    "\n",
    "stopwords.remove(\"no\")\n",
    "#stopwords.remove(\"good\")\n",
    "#stopwords.remove(\"love\")\n",
    "stopwords.remove(\"like\")\n",
    "#stopwords.remove(\"best\")\n",
    "#stopwords.remove(\"!\")\n",
    "print (\"Total number of words in duplicate pair questions :\",len(textp_w))\n",
    "print (\"Total number of words in non duplicate pair questions :\",len(textn_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Word Clouds generated from  duplicate pair question's text __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\n",
    "wc.generate(textp_w)\n",
    "print (\"Word Cloud for Duplicate Question pairs\")\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Word Clouds generated from non duplicate pair question's text __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n",
    "# generate word cloud\n",
    "wc.generate(textn_w)\n",
    "print (\"Word Cloud for non-Duplicate Question pairs:\")\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 3.5.1.2 Pair plot of features ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.shape[0]\n",
    "sns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the token_sort_ratio\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\n",
    "sns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3.5.2 Visualization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dfp_subsampled = df[0:5000]\n",
    "X = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\n",
    "y = dfp_subsampled['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2d = TSNE(\n",
    "    n_components=2,\n",
    "    init='random', # pca\n",
    "    random_state=101,\n",
    "    method='barnes_hut',\n",
    "    n_iter=1000,\n",
    "    verbose=2,\n",
    "    angle=0.5\n",
    ").fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n",
    "\n",
    "# draw the plot in appropriate place in the grid\n",
    "sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\n",
    "plt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne3d = TSNE(\n",
    "    n_components=3,\n",
    "    init='random', # pca\n",
    "    random_state=101,\n",
    "    method='barnes_hut',\n",
    "    n_iter=1000,\n",
    "    verbose=2,\n",
    "    angle=0.5\n",
    ").fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1 = go.Scatter3d(\n",
    "    x=tsne3d[:,0],\n",
    "    y=tsne3d[:,1],\n",
    "    z=tsne3d[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        sizemode='diameter',\n",
    "        color = y,\n",
    "        colorscale = 'Portland',\n",
    "        colorbar = dict(title = 'duplicate'),\n",
    "        line=dict(color='rgb(255, 255, 255)'),\n",
    "        opacity=0.75\n",
    "    )\n",
    ")\n",
    "\n",
    "data=[trace1]\n",
    "layout=dict(height=800, width=800, title='3d embedding with engineered features')\n",
    "fig=dict(data=data, layout=layout)\n",
    "py.iplot(fig, filename='3DBubble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3.6 Featurizing text data with tfidf weighted word-vectors </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# exctract word2vec vectors\n",
    "# https://github.com/explosion/spaCy/issues/1721\n",
    "# http://landinghub.visualstudio.com/visual-cpp-build-tools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid decoding problems\n",
    "df = pd.read_csv(\"/data/szr207/projects/ArqMath/gold/train.tsv\",sep='\\t')\n",
    "\n",
    "# df = pd.read_csv(\"train.csv\" )\n",
    " \n",
    "# encode questions to unicode\n",
    "# https://stackoverflow.com/a/6812069\n",
    "# ----------------- python 2 ---------------------\n",
    "# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# ----------------- python 3 ---------------------\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# merge texts\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False,)\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n",
    "- here we use a pre-trained GLOVE model which comes free with \"Spacy\".  https://spacy.io/usage/vectors-similarity\n",
    "- It is trained on Wikipedia and therefore, it is stronger in terms of word semantics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vectors_web_lg, which includes over 1 million unique vectors.\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "vecs1 = []\n",
    "# https://github.com/noamraph/tqdm\n",
    "# tqdm is used to print the progress bar\n",
    "for qu1 in tqdm(list(df['question1'])):\n",
    "    doc1 = nlp(qu1) \n",
    "    # 384 is the number of dimensions of vectors \n",
    "    mean_vec1 = np.zeros([len(doc1), 384])\n",
    "    for word1 in doc1:\n",
    "        # word2vec\n",
    "        vec1 = word1.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word1)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec1 += vec1 * idf\n",
    "    mean_vec1 = mean_vec1.mean(axis=0)\n",
    "    vecs1.append(mean_vec1)\n",
    "df['q1_feats_m'] = list(vecs1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs2 = []\n",
    "for qu2 in tqdm(list(df['question2'])):\n",
    "    doc2 = nlp(qu2) \n",
    "    mean_vec2 = np.zeros([len(doc2), 384])\n",
    "    for word2 in doc2:\n",
    "        # word2vec\n",
    "        vec2 = word2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word2)]\n",
    "        except:\n",
    "            #print word\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec2 += vec2 * idf\n",
    "    mean_vec2 = mean_vec2.mean(axis=0)\n",
    "    vecs2.append(mean_vec2)\n",
    "df['q2_feats_m'] = list(vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
    "\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
    "df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of nlp features\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data before preprocessing \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions 1 tfidf weighted word2vec\n",
    "df3_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions 2 tfidf weighted word2vec\n",
    "df3_q2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features in nlp dataframe :\", df1.shape[1])\n",
    "print(\"Number of features in preprocessed dataframe :\", df2.shape[1])\n",
    "print(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\n",
    "print(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\n",
    "print(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the final features to csv file\n",
    "if not os.path.isfile('final_features.csv'):\n",
    "    df3_q1['id']=df1['id']\n",
    "    df3_q2['id']=df1['id']\n",
    "    df1  = df1.merge(df2, on='id',how='left')\n",
    "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
    "    result  = df1.merge(df2, on='id',how='left')\n",
    "    result.to_csv('final_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine # database connection\n",
    "import csv\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4. Machine Learning Models </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.1 Reading data from file and storing into sql table </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating db file from csv\n",
    "if not os.path.isfile('train.db'):\n",
    "    disk_engine = create_engine('sqlite:///train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "\n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_db = 'train.db'\n",
    "conn_r = create_connection(read_db)\n",
    "checkTableExists(conn_r)\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to sample data according to the computing power you have\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        # for selecting first 1M rows\n",
    "        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n",
    "        \n",
    "        # for selecting random points\n",
    "        data = pd.read_sql_query(\"SELECT * From data ;\", conn_r)\n",
    "        conn_r.commit()\n",
    "        conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first row \n",
    "data.drop(data.index[0], inplace=True)\n",
    "y_true = data['is_duplicate']\n",
    "data.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we read from sql table each entry was read it as a string\n",
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(data.columns)\n",
    "data = pd.DataFrame(np.array(data.values,dtype=np.float64),columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(int, y_true.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.3 Random train test split( 70:30) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\",X_train.shape)\n",
    "print(\"Number of data points in test data :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "train_distr = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n",
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "test_distr = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function plots the confusion matrices given y_i, y_i_hat.\n",
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    #divid each element of the confusion matrix with the sum of elements in that column\n",
    "    \n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.T = [[1, 3],\n",
    "    #        [2, 4]]\n",
    "    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =1) = [[3, 7]]\n",
    "    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n",
    "    #                           [2/3, 4/7]]\n",
    "\n",
    "    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n",
    "    #                           [3/7, 4/7]]\n",
    "    # sum of row elements = 1\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    #divid each element of the confusion matrix with the sum of elements in that row\n",
    "    # C = [[1, 2],\n",
    "    #     [3, 4]]\n",
    "    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n",
    "    # C.sum(axix =0) = [[4, 6]]\n",
    "    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n",
    "    #                      [3/4, 4/6]] \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [1,2]\n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.4 Building a random model (Finding worst-case log-loss) </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to generate 9 numbers and the sum of numbers should be 1\n",
    "# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n",
    "# ref: https://stackoverflow.com/a/18662466/4084039\n",
    "# we create a output array that has exactly same size as the CV data\n",
    "predicted_y = np.zeros((test_len,2))\n",
    "for i in range(test_len):\n",
    "    rand_probs = np.random.rand(1,2)\n",
    "    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
    "print(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n",
    "\n",
    "predicted_y =np.argmax(predicted_y, axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.4 Logistic Regression with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is sensitive to feature scaling, so did scaling and tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "X_train_sc = scale.fit_transform(X_train)\n",
    "X_test_sc = scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "np.random.seed(45)\n",
    "alpha = np.random.uniform(0.0006,0.006,14)\n",
    "alpha = np.round(alpha,6)\n",
    "alpha.sort()\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.5 Linear SVM with hyperparameter tuning </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y,eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y,eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "np.random.seed(25)\n",
    "alpha = np.random.uniform(0.002,0.03,14)\n",
    "alpha = np.round(alpha,5)\n",
    "alpha.sort()\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y,eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.6 Random Forest </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in estimators:\n",
    "    clf = RFC(n_estimators=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
    "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('estimators') n  \n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Depth = [5,10,12,15,20,25,50]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in Depth:\n",
    "    clf = RFC(n_estimators=100,max_depth=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('Depth = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(Depth,train_scores,label='Train Log Loss')\n",
    "plt.plot(Depth,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('Depth') \n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in estimators:\n",
    "    clf = RFC(n_estimators=i,max_depth=11,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
    "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('estimators') \n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4.7 XGBoost </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in depths:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
    "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('estimators')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "etas = [0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "for i in etas:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=i,n_estimators=350,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('Learning Rate = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(etas,train_scores,label='Train Log Loss')\n",
    "plt.plot(etas,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "alpha = [0.5,1,5,10,50,100,150]\n",
    "for i in alpha:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.65,n_estimators=370,reg_alpha=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('reg_alpha = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(alpha,train_scores,label='Train Log Loss')\n",
    "plt.plot(alpha,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('reg_alpha')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.02,n_estimators=400,n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "predict_y = clf.predict_proba(X_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampled data and did further hyperparam truning because of time constraints. random search for 15 models took around one day and then my system is not responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to sample data according to the computing power you have\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        # for selecting first 1M rows\n",
    "        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r)\n",
    "        \n",
    "        # for selecting random points\n",
    "        data = pd.read_sql_query(\"SELECT * From data LIMIT 100001;\", conn_r)\n",
    "        conn_r.commit()\n",
    "        conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first row \n",
    "data.drop(data.index[0], inplace=True)\n",
    "y_true = data['is_duplicate']\n",
    "data.drop(['Unnamed: 0', 'id','index','is_duplicate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we read from sql table each entry was read it as a string\n",
    "# we convert all the features into numaric before we apply any model\n",
    "cols = list(data.columns)\n",
    "data = pd.DataFrame(np.array(data.values,dtype=np.float64),columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(int, y_true.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, y_train, y_test = train_test_split(data, y_true, stratify=y_true, test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\",X_train.shape)\n",
    "print(\"Number of data points in test data :\",X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in estimators:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
    "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('estimators')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "etas = [0.05,0.1,0.15,0.2,0.25,0.3]\n",
    "for i in etas:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=i,n_estimators=250,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('Learning Rate = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(etas,train_scores,label='Train Log Loss')\n",
    "plt.plot(etas,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "train_scores = []\n",
    "alpha = [0.5,1,5,10,50,100,150]\n",
    "for i in alpha:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.95,n_estimators=250,reg_alpha=i,n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    predict_y = clf.predict_proba(X_train)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('reg_alpha = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(alpha,train_scores,label='Train Log Loss')\n",
    "plt.plot(alpha,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('reg_alpha')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": sp_randint(2,5),\n",
    "              \"learning_rate\":uniform(0,0.2),\n",
    "              \"n_estimators\":sp_randint(200,350),\n",
    "              \"min_child_weight\": sp_randint(2, 8),\n",
    "              \"gamma\": uniform(0,4),\n",
    "              \"subsample\":uniform(0.7,0.3),\n",
    "              \"colsample_bytree\": uniform(0.7,0.3),\n",
    "              \"reg_alpha\":uniform(100,300),\n",
    "              \"reg_lambda\":uniform(100,300)}\n",
    "\n",
    "model_rs_xgb = RandomizedSearchCV(xgb.XGBClassifier(n_jobs=-1,random_state=25), param_distributions=param_dist,\n",
    "                                   n_iter=30,scoring='neg_log_loss',cv=5,n_jobs=-1)\n",
    "model_rs_xgb.fit(X_train,y_train)\n",
    "pickle.dump(model_rs_xgb,open('model_rs_xgb.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scores = []\n",
    "idx = 0\n",
    "for i in model_rs_xgb.grid_scores_:\n",
    "    dict_score = []\n",
    "    dict_score.append(i[0]['n_estimators'])\n",
    "    dict_score.append(i[0]['max_depth'])\n",
    "    dict_score.append(i[0]['subsample'])\n",
    "    dict_score.append(i[0]['min_child_weight'])\n",
    "    dict_score.append(i[0]['learning_rate'])\n",
    "    dict_score.append(i[0]['reg_alpha'])\n",
    "    dict_score.append(i[0]['reg_lambda'])\n",
    "    dict_score.append(i[0]['gamma'])\n",
    "    dict_score.append(i[0]['colsample_bytree'])\n",
    "    dict_score.append(-i[1])\n",
    "    dict_score.append((np.abs(i[2]).std()))\n",
    "    dict_score.append(-model_rs_xgb.cv_results_['mean_train_score'][idx])\n",
    "    dict_scores.append(dict_score)\n",
    "    idx = idx + 1\n",
    "scores_df = pd.DataFrame(dict_scores,columns=['n_estimators','depth','subsample','min_child_weight',\n",
    "                                               'learning_rate','reg_alpha','reg_lambda','gamma',\n",
    "                                               'colsample_bytree','Test_score',\n",
    "                                               'Test_std','Train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values('Test_score').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=4,learning_rate=0.078998,n_estimators=312,\n",
    "                        min_child_weight=7,subsample=0.862849,\n",
    "                        reg_alpha=151.329530,reg_lambda=368.527334,\n",
    "                        colsample_bytree=0.889555,gamma=2.475153,n_jobs=-1)\n",
    "clf.fit(X_train,y_train)\n",
    "predict_y = clf.predict_proba(X_test)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Tf-Idf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[0:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
    "\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3 = dfnlp[['id','question1','question2']]\n",
    "duplicate = dfnlp.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so for Tf-Idf Features i am combining question1 and question2, then getting Tf-Idf for for Train and transforming test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.fillna(' ')\n",
    "df4 = pd.DataFrame()\n",
    "df4['Text'] = df3.question1 + ' ' + df3.question2\n",
    "df4['id'] = df3.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining question1 and question2, then getting Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['id']=df1['id']\n",
    "df4['id']=df1['id']\n",
    "df5  = df1.merge(df2, on='id',how='left')\n",
    "final  = df5.merge(df4, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf,X_test_tf, y_train_tf, y_test_tf = train_test_split(final,duplicate, stratify=y_true, test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,3),max_features=200000,min_df=0.000032)\n",
    "train_tfidf = tfidf_vect.fit_transform(X_train_tf.Text)\n",
    "test_tfidf = tfidf_vect.transform(X_test_tf.Text)\n",
    "print('No of Tfidf features',len(tfidf_vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = X_train_tf.drop('Text',axis=1)\n",
    "X_test_tf = X_test_tf.drop('Text',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train1 = hstack((X_train_tf.values,train_tfidf))\n",
    "X_test1 = hstack((X_test_tf.values,test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler(with_mean=False)\n",
    "X_train_sc = scale.fit_transform(X_train1)\n",
    "X_test_sc = scale.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "alpha = np.random.uniform(0.05,0.5,14)\n",
    "alpha = np.round(alpha,4)\n",
    "alpha.sort()\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_sc, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_sc, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_sc)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it was giving some good scores but it seems to be some overfitting in this.  \n",
    "i think it may be because of feature scaling for all the data including tfidf values so i tried without feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train1, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test1)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train1, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train1)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test1)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i decresed overfittig but bias increased.\n",
    "Tought like i will scale features otherthan Tf-Idf and Tf-Idf was already coming with l2 normalization. so tried with this format below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X_train_some = scale.fit_transform(X_train_tf)\n",
    "X_test_some = scale.transform(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train2 = hstack((X_train_some,train_tfidf))\n",
    "X_test2 = hstack((X_test_some,test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train2, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test2)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train2, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it seems to be good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "alpha = np.random.uniform(0.000002,0.00003,14)\n",
    "alpha = np.round(alpha,8)\n",
    "alpha.sort()\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train2, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test2)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train2, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with Hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train2, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test2)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train2, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "alpha = np.random.uniform(0.000002,0.00003,14)\n",
    "alpha = np.round(alpha,8)\n",
    "alpha.sort()\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train2, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test2)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train2, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test2)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With some others features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
    "\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop(sent):\n",
    "    sent  = str(sent)\n",
    "    if sent == None:\n",
    "        return ' '\n",
    "    if sent==np.nan:\n",
    "        return ' '\n",
    "    if sent == 'NaN':\n",
    "        return ' '\n",
    "    z = [i for i in sent.split() if i not in STOP_WORDS]\n",
    "    return ' '.join(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnlp['question1'] = dfnlp.question1.apply(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnlp['question2'] = dfnlp.question2.apply(remove_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"glove.840B.300d.txt\", word2vec_output_file=\"glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2,model):\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    s1 = s1.split()\n",
    "    s2 = s2.split()\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://proceedings.mlr.press/v37/kusnerb15.pdf i read about word mover distance and after that i calculated some distances from avg word vectors as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnlp['Word_Mover_Dist'] = dfnlp.apply(lambda x: wmd(x['question1'], x['question2'],glove_model), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the avg-w2v for each sentence/review is stored in this list\n",
    "def avg_w2v(list_of_sent,model,d):\n",
    "    '''\n",
    "    Returns average of word vectors for\n",
    "    each sentance with dimension of model given\n",
    "    '''\n",
    "    sent_vectors = []\n",
    "    for sent in list_of_sent: # for each review/sentence\n",
    "        doc = [word for word in sent if word in model.wv.vocab]\n",
    "        if doc:\n",
    "            sent_vec = np.mean(model.wv[doc],axis=0)\n",
    "        else:\n",
    "            sent_vec = np.zeros(d)\n",
    "        sent_vectors.append(sent_vec)\n",
    "    return sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into lists\n",
    "list_of_question1=[]\n",
    "for sent in dfnlp.question1.values:\n",
    "    list_of_question1.append(sent.split())\n",
    "list_of_question2=[]\n",
    "for sent in dfnlp.question2.values:\n",
    "    list_of_question2.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg word 2 vec\n",
    "avgw2v_q1 = avg_w2v(list_of_question1,glove_model,300)\n",
    "avgw2v_q2 = avg_w2v(list_of_question2,glove_model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting as df\n",
    "df_avgw2v = pd.DataFrame()\n",
    "df_avgw2v['q1_vec'] = list(avgw2v_q1)\n",
    "df_avgw2v['q2_vec'] = list(avgw2v_q2)\n",
    "df_q1 = pd.DataFrame(df_avgw2v.q1_vec.values.tolist())\n",
    "df_q2 = pd.DataFrame(df_avgw2v.q2_vec.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing soma distances and calculating\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock,canberra, euclidean, minkowski\n",
    "dfnlp['dist_cosine'] = [cosine(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "dfnlp['dist_cityblock'] = [cityblock(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "dfnlp['dist_canberra'] = [canberra(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "dfnlp['dist_euclidean'] = [euclidean(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]\n",
    "dfnlp['dist_minkowski'] = [minkowski(x, y) for (x, y) in zip(avgw2v_q1,avgw2v_q2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling na values with 0  for cosine distance\n",
    "dfnlp.dist_cosine = dfnlp.dist_cosine.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merzing all df\n",
    "df_q1.reset_index(inplace=True)\n",
    "df_q2.reset_index(inplace=True)\n",
    "df_q1['index'] = df_q2['index']\n",
    "df_avgw2v_final = df_q1.merge(df_q2, on='index',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for final df\n",
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##merging all \n",
    "df1.id = df_avgw2v.index\n",
    "df2.id = df_avgw2v.index\n",
    "df_temp = df1.merge(df2,on='id',how='left')\n",
    "df_final = df_temp.merge(df_avgw2v_final,left_on='id',right_on='index',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving to disk\n",
    "df_final.to_csv('df_final_avg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('df_final_avg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max no after inf is 13.45 so imputed infinity with 30\n",
    "df_final.Word_Mover_Dist = df_final.Word_Mover_Dist.apply(lambda x: 30 if x == np.inf else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set of values\n",
    "np.sort(list(set(df_final.Word_Mover_Dist.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df_final.shape[0]\n",
    "sns.pairplot(df_final[['Word_Mover_Dist', 'dist_cosine', 'dist_cityblock', \n",
    "                       'dist_canberra','dist_euclidean', 'is_duplicate']][0:n], \n",
    "             hue='is_duplicate', vars=['Word_Mover_Dist', 'dist_cosine', 'dist_cityblock', 'dist_canberra','dist_euclidean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing dependent varible\n",
    "duplicate = df_final.is_duplicate\n",
    "df_final = df_final.drop(['id','is_duplicate','index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train,X_test, y_train, y_test = train_test_split(df_final,duplicate, stratify=duplicate, test_size=0.3,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "train_distr = Counter(y_train)\n",
    "train_len = len(y_train)\n",
    "print(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\n",
    "print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\n",
    "test_distr = Counter(y_test)\n",
    "test_len = len(y_test)\n",
    "print(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_col = ['cwc_min', 'cwc_max', 'csc_min', 'csc_max', 'ctc_min', 'ctc_max', 'last_word_eq', \n",
    "             'first_word_eq', 'abs_len_diff', 'mean_len', 'token_set_ratio', 'token_sort_ratio',\n",
    "             'fuzz_ratio', 'fuzz_partial_ratio', 'longest_substr_ratio', 'Word_Mover_Dist',\n",
    "             'dist_cosine', 'dist_cityblock', 'dist_canberra', 'dist_euclidean', \n",
    "             'dist_minkowski', 'freq_qid1', 'freq_qid2', 'q1len', 'q2len', 'q1_n_words', \n",
    "             'q2_n_words', 'word_Common', 'word_Total', 'word_share', 'freq_q1+q2', 'freq_q1-q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale = X_train[scale_col]\n",
    "X_test_scale = X_test[scale_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = X_train.drop(scale_col,axis=1)\n",
    "X_test_w2v = X_test.drop(scale_col,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "X_train_sc = scale.fit_transform(X_train_scale)\n",
    "X_test_sc = scale.transform(X_test_scale)\n",
    "X_train_sc = pd.DataFrame(X_train_sc,columns=X_train_scale.columns)\n",
    "X_test_sc = pd.DataFrame(X_test_sc,columns=X_test_scale.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final train and test vectors after scaling of normal features \n",
    "X_train_fi = pd.DataFrame(np.hstack((X_train_sc.values,X_train_w2v.values)),columns=df_final.columns)\n",
    "X_test_fi = pd.DataFrame(np.hstack((X_test_sc.values,X_test_w2v.values)),columns=df_final.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_fi, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_fi, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "alpha = np.random.uniform(0.0005,0.005,14)\n",
    "alpha = np.round(alpha,6)\n",
    "alpha.sort()\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_fi, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_fi, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n",
    "\n",
    "# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# ------------------------------\n",
    "# default parameters\n",
    "# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n",
    "# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n",
    "# class_weight=None, warm_start=False, average=False, n_iter=None)\n",
    "\n",
    "# some of methods\n",
    "# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n",
    "# predict(X)\tPredict class labels for samples in X.\n",
    "\n",
    "#-------------------------------\n",
    "# video link: \n",
    "#------------------------------\n",
    "\n",
    "\n",
    "log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(X_train_fi, y_train)\n",
    "    predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "    log_error_array.append(log_loss(y_test, predict_y, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha, log_error_array,c='g')\n",
    "for i, txt in enumerate(np.round(log_error_array,3)):\n",
    "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\n",
    "plt.grid()\n",
    "plt.title(\"Cross Validation Error for each alpha\")\n",
    "plt.xlabel(\"Alpha i's\")\n",
    "plt.ylabel(\"Error measure\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(X_train_fi, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_train_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y,eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(X_test_fi)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y,eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "print(\"Total number of data points :\", len(predicted_y))\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in estimators:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=i,n_jobs=-1)\n",
    "    clf.fit(X_train_fi,y_train)\n",
    "    predict_y = clf.predict_proba(X_train_fi)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test_fi)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "#plt.plot(estimators,train_scores,label='Train Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.12,n_estimators=600,\n",
    "                        min_child_weight=5,\n",
    "                        reg_alpha=150,reg_lambda=350,n_jobs=-1)\n",
    "clf.fit(X_train_fi,y_train)\n",
    "predict_y = clf.predict_proba(X_test_fi)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_train_fi.columns)[np.argsort(clf.feature_importances_)[::-1]][0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained XGBoost on data dropping avg word vectors with below columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [100,150,200,300,400,600,800]\n",
    "test_scores = []\n",
    "train_scores = []\n",
    "for i in estimators:\n",
    "    clf = xgb.XGBClassifier(max_depth=3,learning_rate=0.1,n_estimators=i,n_jobs=-1)\n",
    "    clf.fit(X_train_scale,y_train)\n",
    "    predict_y = clf.predict_proba(X_train_scale)\n",
    "    log_loss_train = log_loss(y_train, predict_y, eps=1e-15)\n",
    "    train_scores.append(log_loss_train)\n",
    "    predict_y = clf.predict_proba(X_test_scale)\n",
    "    log_loss_test = log_loss(y_test, predict_y, eps=1e-15)\n",
    "    test_scores.append(log_loss_test)\n",
    "    print('estimators = ',i,'Train Log Loss ',log_loss_train,'Test Log Loss ',log_loss_test)\n",
    "plt.plot(estimators,train_scores,label='Train Log Loss')\n",
    "plt.plot(estimators,test_scores,label='Test Log Loss')\n",
    "plt.xlabel('estimators')\n",
    "plt.ylabel('Log Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\"max_depth\": sp_randint(2,5),\n",
    "              \"learning_rate\":uniform(0,0.25),\n",
    "              \"n_estimators\":sp_randint(300,600),\n",
    "              \"min_child_weight\": sp_randint(2, 8),\n",
    "              \"gamma\": uniform(0,4),\n",
    "              \"subsample\":uniform(0.7,0.3),\n",
    "              \"colsample_bytree\": uniform(0.7,0.3),\n",
    "              \"reg_alpha\":uniform(100,300),\n",
    "              \"reg_lambda\":uniform(100,300)}\n",
    "\n",
    "model_rs_xgb1 = RandomizedSearchCV(xgb.XGBClassifier(n_jobs=-1,random_state=25), param_distributions=param_dist,\n",
    "                                   n_iter=30,scoring='neg_log_loss',cv=5,n_jobs=-1)\n",
    "model_rs_xgb1.fit(X_train_scale,y_train)\n",
    "pickle.dump(model_rs_xgb1,open('model_rs_xgb1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_scores = []\n",
    "idx = 0\n",
    "for i in model_rs_xgb1.grid_scores_:\n",
    "    dict_score = []\n",
    "    dict_score.append(i[0]['n_estimators'])\n",
    "    dict_score.append(i[0]['max_depth'])\n",
    "    dict_score.append(i[0]['subsample'])\n",
    "    dict_score.append(i[0]['min_child_weight'])\n",
    "    dict_score.append(i[0]['learning_rate'])\n",
    "    dict_score.append(i[0]['reg_alpha'])\n",
    "    dict_score.append(i[0]['reg_lambda'])\n",
    "    dict_score.append(i[0]['gamma'])\n",
    "    dict_score.append(i[0]['colsample_bytree'])\n",
    "    dict_score.append(-i[1])\n",
    "    dict_score.append((np.abs(i[2]).std()))\n",
    "    dict_score.append(-model_rs_xgb1.cv_results_['mean_train_score'][idx])\n",
    "    dict_scores.append(dict_score)\n",
    "    idx = idx + 1\n",
    "scores_df = pd.DataFrame(dict_scores,columns=['n_estimators','depth','subsample','min_child_weight',\n",
    "                                               'learning_rate','reg_alpha','reg_lambda','gamma',\n",
    "                                               'colsample_bytree','Test_score',\n",
    "                                               'Test_std','Train_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values('Test_score').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in my view 2nd line (28) is beeter score than forst beacuse of train test scores and test standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score params')\n",
    "scores_df.loc[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(max_depth=4,learning_rate=0.131131,n_estimators=500,\n",
    "                        min_child_weight=6,\n",
    "                        reg_alpha=119.704012,reg_lambda=115.715236,\n",
    "                        gamma=3.768808,colsample_bytree=0.911753,n_jobs=-1)\n",
    "clf.fit(X_train_scale,y_train)\n",
    "predict_y = clf.predict_proba(X_test_scale)\n",
    "print(\"The test log loss is:\",log_loss(y_test, predict_y, eps=1e-15))\n",
    "predicted_y =np.argmax(predict_y,axis=1)\n",
    "plot_confusion_matrix(y_test, predicted_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
