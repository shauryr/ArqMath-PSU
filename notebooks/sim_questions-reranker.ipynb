{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Questions search\n",
    "\n",
    "\n",
    "The idea is that similar questions will have similar answers. So once we can find similar questions then we can just rank there answers ?\n",
    "\n",
    "Strategy :\n",
    "\n",
    "1. Search \"topics\" in question index.\n",
    "\n",
    "2. get q_n (some number which can yeild 1000 answers) similar questions\n",
    "\n",
    "3. Now rank these 1000 using RoBERTa-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import jsonlines\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from elasticsearch import Elasticsearch\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.31 doubt implication logical reasoning. evident truth table $p \\to q$   $p$ false $q$ true, $p \\to q$ true.  instances convince truth value.  example:  $p$: quadrilateral cyclic  $q$: opposite angles supplementary  $p$ false $q$ true, $p \\to q$ true? \n",
      "=================================\n",
      "A.78 $\\sum_{n=1}^{\\infty} a_n$ converges absolutely prove $\\sum_{n=1}^{\\infty} a_n^2$ converges. studying real analysis.i proof proofs exist. but, wrong proof.thanks advance     1.we show satisfies cauchy criterion series  2.consider arbitrary $\\epsilon &gt; 0$  3.since $\\sum a_n$ converges absolutely  exists s.t forall $n &gt; \\geq n$ s.t $|\\sum_{k=m+1}^{n} |a_k|| = \\sum_{k=m+1}^{n} |a_k| &lt; \\sqrt{\\epsilon}$  4.we show suffices  5.consider arbitrary $n &gt; \\geq n$  $|\\sum_{k=m+1}^{n} a_k^2|  =  \\sum_{k=m+1}^{n} a_k^2 \\leq  (\\sum_{k=m+1}^{n} |a_k|)^2 &lt; \\sqrt{\\epsilon}^2 = \\epsilon$  6.so conclude $\\sum a_n^2$ converges \n",
      "=================================\n",
      "A.101 prove squared equal 2 induction.. asked prove induction $n^2\\leq 2^n$, told true $ \\forall n\\in \\mathbb{n},n&gt;3$   found proof, stuck half-way there. taking base case $n=4$ tested it, resulted true. assumed true number $k$, $n=k$ $k^2\\leq 2^k$, attempted prove   $(k+1)^2 \\leq 2^{k+1}$  attempted prove this. all, started assumption.  $=k^2\\leq 2^k$  $=2k^2\\leq2^{k+1}$  prove $(k+1)^2 \\leq 2k^2$, imply thesis, i.e. $(k+1)^2\\leq2^{k+1}$. effort:  $(k+1)^2â‰¤2k^2$  $=k^2+2k+1\\leq2k^2$  $=2k+1\\leq k^2$  (by assumption)  $=2k+1\\leq 2^k$   simplified it, prove true; is, prove $(k+1)^2 \\leq 2k^2$. base case $n=4$ deed satisfies inequality. assume true number $j, k=j$ prove it. failed prove this, steps wrong. reasoning okay? its, prove $2k+1\\leq 2^k$? advance. \n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "class Topic:\n",
    "    \"\"\"\n",
    "    This class shows a topic for task 1. Each topic has an topic_id which is str, a title and question which\n",
    "    is the question body and a list of tags.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic_id, title, question, tags):\n",
    "        self.topic_id = topic_id\n",
    "        self.title = title\n",
    "        self.question = question\n",
    "        self.lst_tags = tags\n",
    "\n",
    "\n",
    "class TopicReader:\n",
    "    \"\"\"\n",
    "    This class takes in the topic file path and read all the topics into a map. The key in this map is the topic id\n",
    "    and the values are Topic which has 4 attributes: id, title, question and list of tags for each topic.\n",
    "\n",
    "    To see each topic, use the get_topic method, which takes the topic id and return the topic in Topic object and\n",
    "    you have access to the 4 attributes mentioned above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, topic_file_path):\n",
    "        self.__map_topics = self.__read_topics(topic_file_path)\n",
    "\n",
    "    def __read_topics(self, topic_file_path):\n",
    "        map_topics = {}\n",
    "        tree = ET.parse(topic_file_path)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            topic_id = child.attrib['number']\n",
    "            title = child[0].text\n",
    "            question = child[1].text\n",
    "            lst_tag = child[2].text.split(\",\")\n",
    "            map_topics[topic_id] = Topic(topic_id, title, question, lst_tag)\n",
    "        return map_topics\n",
    "\n",
    "    def get_topic(self, topic_id):\n",
    "        if topic_id in self.__map_topics:\n",
    "            return self.__map_topics[topic_id]\n",
    "        return None\n",
    "\n",
    "def remove_stop(query):\n",
    "    with open('../englishST.txt') as f:\n",
    "        all_stopwords = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    all_stopwords = [x.strip() for x in all_stopwords] \n",
    "    text_tokens = query.split(' ')\n",
    "    query = [word for word in text_tokens if not word in all_stopwords]\n",
    "    query = ' '.join(query)\n",
    "    return query\n",
    "\n",
    "def remove_punct(my_str):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    # To take input from the user\n",
    "    # my_str = input(\"Enter a string: \")\n",
    "\n",
    "    # remove punctuation from the string\n",
    "    no_punct = \"\"\n",
    "    for char in my_str:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "\n",
    "    # display the unpunctuated string\n",
    "    return no_punct\n",
    "\n",
    "es = Elasticsearch(['http://csxindex05:9200/'], verify_certs=True)\n",
    "queries = []\n",
    "#\"In this example, the title and the question body of topic with id A.1 is printed.\"\n",
    "topic_file_path = \"/data/szr207/dataset/ArqMath/Task1/Sample Topics/Task1_Samples_V2.0.xml\"\n",
    "topic_reader = TopicReader(topic_file_path)\n",
    "dict_q_a = defaultdict(list)\n",
    "\n",
    "for topic_id in ['A.31','A.78', 'A.101']:\n",
    "#     topic_id = \"A.31\"\n",
    "    title = re.sub('<[^<]+?>', '', topic_reader.get_topic(topic_id).title)\n",
    "    body = topic_reader.get_topic(topic_id).question\n",
    "    body_pro = re.sub('<[^<]+?>', '', body)\n",
    "    query = title + '. ' + body_pro\n",
    "    queries.append(query)\n",
    "    query = query.lower()\n",
    "    query = remove_stop(query)\n",
    "    \n",
    "#     print(topic_reader.get_topic(topic_id).lst_tags)\n",
    "    print(topic_id, query)\n",
    "#     print(body_pro)\n",
    "    print(\"=================================\")\n",
    "    body = {\n",
    "        \"size\": 500,\n",
    "        \"query\": {\n",
    "               \"more_like_this\" : {\n",
    "            \"fields\" : [\"title^3\", \"body\"],\n",
    "            \"like\" : query,\n",
    "            \"min_term_freq\" : 1,\n",
    "            \"max_query_terms\" : 12\n",
    "        }\n",
    "#             \"multi_match\": {\n",
    "#                             \"query\": query,\n",
    "#                             \"fields\":  [\"body\",\"title\"],\n",
    "#                             \"type\": \"cross_fields\"\n",
    "#                         }\n",
    "#             \"match\": {\n",
    "#                 \"body\": query\n",
    "#             }\n",
    "         \n",
    "        }\n",
    "    }\n",
    "\n",
    "    res = es.search(index=\"arq_ans_ques\", body=body)\n",
    "    for result in res['hits']['hits']:\n",
    "        dict_q_a[topic_id].append(result['_source']['post_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c669b9958548848432f90acfea3d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394fd4a13d4d4498a402f681e513c9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491b7aecfd59435db0295aa9d2bba607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ques_path = '/data/szr207/dataset/ArqMath/jsons/questions/all.ques.jsonl'\n",
    "\n",
    "dict_ques = {}\n",
    "\n",
    "with jsonlines.open(os.path.join(ques_path)) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            dict_ques[obj['post_id']] = obj\n",
    "            \n",
    "ans_path = '/data/szr207/dataset/ArqMath/jsons/answers/all.ans.jsonl'\n",
    "      \n",
    "dict_ans = {}\n",
    "    \n",
    "with jsonlines.open(os.path.join(ans_path)) as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            dict_ans[obj['post_id']] = obj\n",
    "\n",
    "dict_aid_post = {}\n",
    "with jsonlines.open(os.path.join(ans_path)) as reader:\n",
    "    for obj in tqdm(reader):\n",
    "        if obj['body']:\n",
    "            obj['body'] = re.sub('<[^<]+?>', '',  obj['body'])\n",
    "            dict_aid_post[obj['post_id']] = obj['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3686f1901e416e9b6fbf8d28de2c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2689399",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c861e459107a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlist_other_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mqid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_q_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdict_ques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted_answer_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict_ans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mlist_ans_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_ques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accepted_answer_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdict_ques\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2689399"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "dict_topic_ans = {}\n",
    "\n",
    "# accepted answers can have a separate list and get higher votes\n",
    "\n",
    "for topic_id in dict_q_a:\n",
    "    list_ans_idx = []\n",
    "    list_other_ans = []\n",
    "    for qid in tqdm(dict_q_a[topic_id]):\n",
    "        if dict_ques[qid]['accepted_answer_id'] in dict_ans:\n",
    "            list_ans_idx.append(dict_ques[qid]['accepted_answer_id'])\n",
    "        if dict_ques[qid]['answers']:\n",
    "            for ans in dict_ques[qid]['answers']:\n",
    "                if ans in dict_ans:\n",
    "                    list_other_ans.append(ans)\n",
    "    list_ans_idx.extend(list_other_ans)\n",
    "    dict_topic_ans[topic_id] = list(set(list_ans_idx))\n",
    "    print(len(list_ans_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = defaultdict(list)\n",
    "for topic_id in dict_topic_ans:\n",
    "    score = 1000\n",
    "    for ans in dict_topic_ans[topic_id]:\n",
    "        result_list[topic_id].append((topic_id, ans, score))\n",
    "        score-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('es.500.sim.ques.morelikethis2.dat', 'w') as eval_file:\n",
    "#     for res in result_list:\n",
    "#         count = 1\n",
    "#         for tuples in result_list[res][:1000]:\n",
    "# #             print(tuples)\n",
    "#             eval_file.write(tuples[0]+'\\t'+ '1\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'rr'+'\\n')\n",
    "# #             print(tuples[0]+'\\t'+ '1\\t' + tuples[1] +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'rr')\n",
    "#             count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import *\n",
    "import torch \n",
    "\n",
    "# del feat_ext\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "feat_ext = pipeline(\"feature-extraction\", model=\"/data/szr207/github/transformers/examples/language-modeling/output/checkpoint-3000000\", tokenizer='roberta-base', device=0) #our finetuned model\n",
    "# feat_ext = pipeline(\"feature-extraction\", model=\"shauryr/arqmath-roberta-base-3M\", tokenizer='roberta-base', device=0)\n",
    "# feat_ext = pipeline(\"feature-extraction\", model=\"shauryr/arqmath-roberta-base\", tokenizer='roberta-base', device=0)\n",
    "# tokenizer =  BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# feat_ext = pipeline(\"feature-extraction\", model=\"roberta-base\", tokenizer='roberta-base', device=0)\n",
    "ques_emb = []\n",
    "for query in queries:\n",
    "    ques_emb.append(np.mean(feat_ext(query)[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup):  \n",
    "    # reverse = None (Sorts in Ascending order)  \n",
    "    # key is set to sort using second element of  \n",
    "    # sublist lambda has been used  \n",
    "    tup.sort(key = lambda x: x[2])\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d806c5a57f84a588c37555ee82b34ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1042.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1042\n",
      "A.31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee2c825750447719acacb51586d4a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=650.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "650\n",
      "A.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd51bb52cab7400ea474c2f4c824e44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1043.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1043\n",
      "A.101\n"
     ]
    }
   ],
   "source": [
    "dict_q_idx = {'A.31':0,'A.78':1, 'A.101':2}\n",
    "result_list = []\n",
    "count = 0\n",
    "import math\n",
    "for qid in ['A.31','A.78', 'A.101']:\n",
    "    tup_postid_sim = []\n",
    "    for post_id in tqdm(dict_topic_ans[qid]):\n",
    "#         print(qid,post_id)\n",
    "#         ans_emb = np.mean(feat_ext(reduce_str(dict_aid_post[post_id],300))[0], axis=0)\n",
    "        ans_emb = np.mean(feat_ext(dict_aid_post[post_id])[0], axis=0)\n",
    "#         ans_emb = feat_ext(dict_aid_post[post_id])[0][0]\n",
    "#         print(ans_emb)\n",
    "        result = 1 - spatial.distance.cosine(ques_emb[dict_q_idx[qid]], ans_emb)\n",
    "        if math.isnan(result):\n",
    "            count+=1\n",
    "            print(qid, post_id, count)\n",
    "#             tup_postid_sim.append((qid,post_id,str(1.0)))\n",
    "            continue\n",
    "        tup_postid_sim.append((qid,post_id,result))\n",
    "    \n",
    "    print(len(tup_postid_sim))\n",
    "    print(qid)\n",
    "    result_list.append(Sort_Tuple(tup_postid_sim)[::-1][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('es.mlt.qai.3M.limtfidf.dat', 'w') as eval_file:\n",
    "    for res in result_list:\n",
    "        count = 1\n",
    "        for tuples in res:\n",
    "#             print(tuples)\n",
    "            eval_file.write(tuples[0]+'\\t'+ '1\\t' + str(tuples[1]) +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'rr'+'\\n')\n",
    "#             print(tuples[0]+'\\t'+ '1\\t' + tuples[1] +'\\t'+str(count)+'\\t'+ str(tuples[2])+'\\t'+ 'rr')\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
