A.1	Finding value of $c$ such that the range of the rational function $f(x) = \frac{x^2 + x + c}{x^2 + 2x + c}$ does not contain $[-1, -\frac{1}{3}]$. I am comfortable when I am asked to calculate the range of a rational function, but how do we do the reverse? I came across this problem.  If $$f(x)= \frac{x^2 + x + c}{x^2 + 2x + c}$$ then find the value of $c$ for which the range of $f(x)$ does not contain $[-1, -\frac{1}{3}]$. 
A.2	Solving differential equations of the form $f'(x)=f(x+1)$. How to solve differential equations of the following form:   $\frac{df}{dx} = f(x+1)$ 
A.3	Approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$. I am attempting to resolve the following problem:     Find an approximation to $\sqrt{5}$ correct to an exactitude of $10^{-10}$ using the bisection algorithm.   From what I understand, $\sqrt{5}$ has to be placed in function of $x$ but I am not sure where to go from there.  Also, a function in Mathematica are given to do the calculations in which the function $f(x)$, $a$ and $b$ (from the interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs), the tolerance and the number of iterations. 
A.4	How to compute this combinatoric sum?. I have the sum  $$\sum_{k=0}^{n} \binom{n}{k} k$$  I know the result is $n 2^{n-1}$ but I don't know how you get there. How does one even begin to simplify a sum like this that has binomial coefficients. 
A.5	A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?.    A family has two children. Given that one of the children is a boy, what is the probability that both children are boys?   I was doing this question using conditional probability formula.   Suppose, (1) is the event, that the first child is a boy, and (2) is the event that the second child is a boy.  Then the probability of the second child to be boy given that first child is a boys by formula, $P((2)|(1))=\frac{P((2) \cap (1))}{P((1))}=\frac{P((2))P((1))}{P((1))} = P((2))$ ...since second child to be boy doesn't depend on first child and vice versa. Please provide the detailed solution and correct me if I am wrong.  
A.6	How to calculate mod of number with big exponent. I want to find  $$ 5^{133} \mod 8. $$ I have noticed that $5^n \mod 8 = 5$ when $n$ is uneven and 1 otherwise, which would lead me to say that $5^{133} \mod 8 = 5$ But I don't know how to prove this. How can I prove that this is the case (or find another solution if it is not)? 
A.7	Finding out the remainder of $\frac{11^\text{10}-1}{100}$ using modulus.    If $11^\text{10}-1$ is divided by $100$, then solve for '$x$' of the below term $$11^\text{10}-1 = x \pmod{100}$$   Whatever I tried:  $11^\text{2} \equiv 21 \pmod{100}$.....(1)  $(11^\text{2})^\text{2} \equiv (21)^\text{2} \pmod{100}$  $11^\text{4} \equiv 441 \pmod{100}$  $11^\text{4} \equiv 41 \pmod{100}$  $(11^\text{4})^\text{2} \equiv (41)^\text{2} \pmod{100}$  $11^\text{8} \equiv 1681 \pmod{100}$  $11^\text{8} \equiv 81 \pmod{100}$  $11^\text{8} × 11^\text{2} \equiv (81×21) \pmod{100}$ ......{from (1)}  $11^\text{10} \equiv 1701 \pmod{100} \implies 11^\text{10} \equiv 1 \pmod{100}$  Hence, $11^\text{10} -1 \equiv (1-1) \pmod{100} \implies 11^\text{10} - 1 \equiv 0 \pmod{100}$ and thus we get the value of $x$ and it is $x = 0$ and $11^\text{10}-1$ is divisible by $100$.  But this approach take a long time for any competitive exam or any math contest without using calculator. Any easier process on how to determine the remainder of the above problem quickly? That will be very much helpful for me. Thanks in advance. 
A.8	finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$. Finding value of $\lim_{n\rightarrow \infty}\sqrt[n]{\frac{(27)^n(n!)^3}{(3n)!}}$  what i try  $\displaystyle l=\lim_{n\rightarrow \infty}\bigg(\frac{(27)^n(n!)^3}{(3n)!}\bigg)^{\frac{1}{n}}$  $\displaystyle \ln(l)=\lim_{n\rightarrow \infty}\frac{1}{n}\bigg[n\ln(27)+3\ln(n!)-\ln((3n)!)\bigg]$  How do i solve it help me please  
A.9	Simplifying this series. I need to write the series   $$\sum_{n=0}^N nx^n$$   in a form that does not involve the summation notation, for example $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$. Does anyone have any idea how to do this? I've attempted multiple ways including using generating functions however no luck 
A.10	Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .. Find the values of a>0 for which the improper integral $\int_{0}^{\infty}\frac{\sin x}{x^{a}} $ converges .  Do  I have to expand integrand using series expansion?? 
A.11	What's the cross product in 2 dimensions?. The math book i'm using states that the cross product for two vectors is defined over $R^3$:  $$u = (a,b,c)$$ $$v = (d,e,f)$$  is:  $$u \times v = \begin{vmatrix} \hat{i} &amp; \hat{j} &amp; \hat{k} \\ a &amp; b &amp; c \\ d &amp; e &amp; f \\ \end{vmatrix} $$  and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v.    Out of curiosity, what's the cross product if u and v are defined over $R^2$ instead of $R^3$ instead:  $$u = (a,b)$$ $$v = (d,e)$$  Is there a "degenerate" case for the cross product of $R^2$ instead $R^3$?  like this is some type of 2x2 determinant instead?  for instance if had a parameterization:  $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$  and needed to calculate in $R^2$:  $$ D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$  There are plenty of examples in the book for calculating the determinate D in $R^3$ but none at all for $R^2$ case.  As in:  $$ \iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$  $$ \Phi(u,v)=(2u \cos v,\ \ u \sin v) $$ 
A.12	Finding the roots of a complex number. I was solving practice problems for my upcoming midterm and however I got stuck with this question type.  It is asking me to find all roots and then sketch it.  $(1+i\sqrt{3})^{1/2}$  How do we proceed?  
A.13	How to simplify expression $\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$. How to simplify expression  $$\int_a^b f(x)dx+\int_{f(a)}^{f(b)} f^{-1}(x)dx \ ?$$  The answer is $bf(b)-af(a)$  but I am wondering how to get the answer. 
A.14	Help solving first-order differential equation. I have first-order differential equation $$y=xy'+ \frac{1}{2}(y')^{2}$$ Maybe, with this someone will find way to solve it $$\frac{1}{2}y'(2x+y')=y$$ I thought I can use $x^2+y=t$ for subtitution and when I derivate, I have $t'=2x+y'\\(t'-2x)t'=2t-2x^2$ which is acctualy the same as previous. I don't have idea how to start..  
A.15	Derive the sum of $\sum_{i=1}^n ix^{i-1}$. For the series      $$1 + 2x + 3x^2 + 4x^3 + 5x^4 + ... + nx^{n-1}+... $$    and $x \ne 1, |x| &lt; 1$.  I need to find partial sums and finally, the sum $S_n$ of series. Here is what I've tried:    We can take a series $S_2 = 1 + x + x^2 + x^3 + x^4 + ...$ so that $\frac{d(S_2)}{dx} = S_1$ (source series). For the $|x| &lt; 1$ the sum of $S_2$ (here is geometric progression): $\frac{1-x^n}{1-x} = \frac{1}{1-x}$ $S_1 = \frac{d(S_2)}{dx} = \frac{d(\frac{1}{1-x})}{dx} = \frac{1}{(1-x)^2}$   But this answer is incorrect. Where is my mistake? Thank you. 
A.16	Finding $ \int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}dx$.    Calculate   $$\int_0^1\frac{\ln(1+x)\ln(1-x)}{1+x}\,dx$$   My try :   Let : $$I(a,b)=\int_0^1\frac{\ln(1-ax)\ln(1+bx)}{1+x}\,dx$$  Then compute $\frac{d^2 I(a,b)}{dadb}$.  I'm happy to see ideas in order to kill this integral. 
A.17	Calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $\frac{e^{iz}}{z}$. I want to calculate $\int _{x=0}^{\infty} \frac{\sin(x)}{x}$ with the function $f(z) = \frac{e^{iz}}{z}$.  I thought about using the closed path $\Gamma = \gamma _1 + \gamma _R + \gamma _2 + \gamma _{\epsilon}$, when:  $\gamma_1 (t) = t, t \in [i\epsilon, iR]$  $\gamma_R (t) = Re^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$  $\gamma_2 (t) = t, t \in [-iR, -i\epsilon]$  $\gamma_{\epsilon} (t) = \epsilon e^{it}, t \in [-\frac{\pi}{2}, \frac{\pi}{2}]$  I use the fact that $\frac{\sin(x)}{x}$ is an even function and has an anti derivative, so the integral on a closed path is zero.  I managed to show that $\int_{\gamma _{\epsilon}} f = -i\pi$ when $\epsilon \to 0$.  However I am struggling to show that $\int_{\gamma _R} f = 0$ when $R \to \infty$  Help would be appreciated 
A.18	Evaluate $\lim_{n \rightarrow \infty } \frac {[(n+1)(n+2)\cdots(n+n)]^{1/n}}{n}$. Evaluate $$\lim_{n \rightarrow \infty~} \dfrac {[(n+1)(n+2)\cdots(n+n)]^{\dfrac {1}{n}}}{n}$$ using Cesáro-Stolz theorem.  I know there are many question like this, but i want to solve it using Cesáro-Stolz method and no others.  I took log and applied Cesáro-Stolz, I get $$\log{2}+n\log\cfrac{n}{n+1}$$  Which gives me answer as $\frac{2}{e}$ . But answer is $\frac{4}{e}$. Could someone help?.  Edit:  On taking log,  $$\lim_{n \to \infty} \frac{-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)}{n} \\= \lim_{n \to \infty} \left(-(n+1)\log (n+1) + \sum\limits_{k=1}^{n+1} \log \left(k+n\right)\right) - \left(-n\log n + \sum\limits_{k=1}^{n} \log \left(k+n\right)\right) \\ = \lim_{n \to \infty} \log \frac{2n+1}{n+1} - n\log \left(1+\frac{1}{n}\right) = \log 2 - 1$$ Which gives $2/e$ 
A.19	Greatest common factor of $ p^4-1$. I was asked to find the greatest common factor of $p^4-1$ for all primes > 5, First I got the value of $7^4 - 1$ which has divisors of $2^4* 3 *5*2$ and $11^4 - 1$ which has divisors $2^4 *3 * 5*61$ which has a GCF of $2^4*3*5$ I can prove that $p^4 - 1$ is divisible by 3 and 5 by casework and 8  by $(p^2+1)(p-1)(p+1)$ are even integers, but I don't know how to prove divisibility of $2^4$, I do not want to bash it since we must check about 7 numbers to prove its divisibility by assigning $16n + x$ where x &lt;16 
A.20	Calculate all $n \in \Bbb N \setminus \{41\}$ such that $\phi(n)=40$?.    I'm looking for an $n \in \Bbb N$ for which $\phi(n) = 40$ where $\phi$ is a Euler-Totient Function    I already found one, namely, $n=41$  How the calculate the $n's$? 
A.21	Finding the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s). I'm continuing on my journey learning about modular arithmetic and got confused with this question:  Find the last two digits of $9^{9^{9^{…{^9}}}}$ (nine 9s). The phi function is supposed to be used in this problem and so far this is what I've got:  $9^{9^{9^{…{^9}}}} ≡ x (\text{mod } 100)$ Where $0 ≤ x ≤ 100$  $9^{9^{9^{…{^9}}}} \text{ (nine 9s) }= 9^a$ In order to know $9^a (\text{mod } 100)$, we need to know $a (\text{mod } \phi(100))$ As $\phi(100)= 40$, we get $a = b (\text{mod } 40)$  $9^{9^{9^{…{^9}}}} \text{ (eight 9s) }= 9^b$ In order to know $9^b (\text{mod } 40)$, we need to know $b (\text{mod } \phi(40))$ As $\phi(40)= 16$, we get $b = c ( \text{mod }16)$  $9^{9^{9^{…{^9}}}}\text{ (seven 9s) }= 9^c $ In order to know $9^c (mod 16)$, we need to know $c (\text{mod } phi(16))$ as $\phi(16)= 8 $ we need to find $c (\text{mod } 8)$  As $9 = 1 (\text{mod } 8)$ $c = 1 (\text{mod } 8)$  I feel like I might have made a mistake somewhere along the way because I'm having a lot of trouble stitching it all back together in order to get a value for the last two digits. Could anyone please help me with this? Thank you! 
A.22	Find number of d's that satisfies $d, d+1, d+2... = N$ for an $N$. I have a challenge about a cat in a trip where he can walk in the way of $d, d+1, d+2...$ and the sum of that should give $N$, given an $N$, how many ways of chosing $d$ are posible?  Example:  $N=30$ -> $Ans=3$  $d_1=4; d_2=6; d_3=8$   For $d_1: 4 + 5 + 6 + 7 + 8 = 30$  Edit:  Another way to see it is: How many subsets in the sumation up to N are posible in the way $(\sum (d+n) - \sum(d-1))$=N 
A.23	How do i find the lcm. Qn: If the product of two integers is  $2^7 \cdot 3^8 \cdot 5^2 \cdot 7^{11}$ and their greatest common divisor is $2^3 \cdot 3^4 \cdot 5$, what is their least common multiple?  I have issue with this question please help me solve it.  I tried assuming that lcm is $x$ =. Then,        Gcd $\cdot x = 2^3  \cdot 3^4 \cdot 5x$. And, product factors /Gcd $x$ 
A.24	Is this the only way to evaluate $\sqrt{2i-1}?$. work out the $\sqrt{2i-1}?$  $2i-1=(a+bi)^2$  $a^2+2abi-b^2$  $a^2-b^2=-1$  $2ab=2$    $a^2=b^{-2}$  $b^{-2}-b^2=-1$  $-b^{4}+1=-1$  $b^4=2$  $b=\sqrt[4]{2}$  Can we solve $\sqrt{2i-1}$ in another way? 
A.25	What can be P(0), when $P(x^2+1)=(P(x))^2+1$ and P(x) is polynomial?. What can be $P(0)$, when $P(x^2+1)=(P(x))^2+1$ and $P(x)$ is polynomial?  Let $P(0)=0$, then $P(1)=1$, $P(2)=2$, $P(5)=5$, $P(26)=26$, $P(677)=677$ ... and so on. Then $P(x)=x$, because all the points on $y=P(x)$ are $y=x$.  If $P(0)=2$, then $P(x)=(x^2+1)^2+1$ for the same reason.  But when $P(0)=3$, we have that $\lim_{x→∞}\log_xP(x)$ does not converge into an integer. So I think $P(x)$ cannot be a polynomial.  Then what are the values of $P(0)$ that makes $P(x)$ polynomial? 
A.26	How to solve an indefinite integral using the Taylor series?. I am trying to show that the following integral is convergent but not absolutely.     $$\int_0^\infty\frac{\sin x}{x}dx.$$   My attempt:     I first obtained the taylor series of $\int_0^x\frac{sin x}{x}dx$ which is as follows:   $$x-\frac{x^3}{3 \times 3!}+\frac{x^5}{5\times5!}-\frac{x^7}{7 \times 7!}+\cdots = \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!} $$   Now $\int_0^\infty\frac{\sin x}{x}dx=\lim_{x\to \infty} \sum_{n=0}^\infty (-1)^n\frac{x^{(2n+1)}}{(2n+1) \times (2n+1)!}$  and I got stuck here! What is the next step? 
A.27	What is the value of $e^{3i \pi /2}$?. When solving for the value, we know that $e^{\pi i}=-1$ . I am confused as to what is the right answer when you evaluate this.I am getting two possible answers: $e^{3\pi i/2}$ = $(e^{\pi i})^{3/2}$ so this could be $(\sqrt{-1})^3=i^3=-i$ or it could be $\sqrt{(-1)^3}=\sqrt{-1}=i$. Which one is the correct answer, and where am I going wrong? Thanks. 
A.28	If $\sin(18^\circ)=\frac{a + \sqrt{b}}{c}$, then what is $a+b+c$?. If $\sin(18)=\frac{a + \sqrt{b}}{c}$ in the simplest form, then what is $a+b+c$? $$ $$ Attempt: $\sin(18)$ in a right triangle with sides $x$ (in front of corner with angle $18$ degrees), $y$, and hypotenuse $z$, is actually just $\frac{x}{z}$, then $x = a + \sqrt{b}, z = c$. We can find $y$ as $$ y = \sqrt{c^{2}- (a + \sqrt{b})^{2}} $$ so we have  $$ \cos(18) = \frac{y}{z} = \frac{\sqrt{c^{2}- (a + \sqrt{b})^{2}}}{c}$$  I also found out that $$b = (c \sin(18) - a)^{2} = c^{2} \sin^{2}(18) - 2ac \sin(18) + a^{2}$$  I got no clue after this.    The solution says that $$ \sin(18) = \frac{-1 + \sqrt{5}}{4} $$  I gotta intuition that we must find $A,B,C$ such that $$ A \sin(18)^{2} + B \sin(18) + C = 0 $$  then $\sin(18)$ is a root iof $Ax^{2} + Bx + C$, and $a = -B, b = B^{2} - 4AC, c = 2A$.    Totally different. This question is not asking to prove that $sin(18)=(-1+\sqrt{5})/4$, that is just part of the solution. 
A.29	Dividing Complex Numbers by Infinity. My PreCalculus teacher recently reviewed the properties of limits with us before our test and stated that any real number divided by infinity equals zero. This got me thinking and I asked them whether a complex number (i.e. $3+2i$ or $-4i$) divided by infinity would equal zero.   This completely stumped them and I was unable to get an answer. After doing some theoretical calculation, knowing that $i=\sqrt{-1}$, I calculated that a complex number such as $\frac{5i}{\infty}=0$ since  $$\frac{5}{\infty}\cdot \frac{\sqrt{-1}}{\infty} = 0\cdot 0 = 0,$$  using properties utilized with real numbers that would state that $\frac{5x}{\infty} = 0$ since $$\frac{5}{\infty}\cdot \frac{x}{\infty} = 0\cdot 0 = 0.$$ Is this theoretical calculation correct or is there more to the concept than this? 
A.30	Find $a^3+b^3+c^3-3abc$ (binomial theorem).    $$a=\sum_{n=0}^\infty\frac{x^{3n}}{(3n)!}\\b=\sum_{n=1}^\infty\frac{x^{3n-2}}{(3n-2)!}\\c=\sum_{n=1}^\infty\frac{x^{3n-1}}{(3n-1)!}$$Find $a^3+b^3+c^3-3abc$:      $(a)\ 1$      $(b)\ 0$      $(c)-1$      $(d)-2$   Please help me solve this question.  I added $a,b$ and $c$. It gives me the expansion of $e^x$.  But i dont know how to use it. 
A.32	Are definitions axioms?. I just want to ask a very elementary question.  When we introduce a "definition" in a first order logical system. For example when we say   Define: $Empty(x) \iff \not \exists y (y \in x) $   Isn't that definition itself an "axiom", call it a definitional axiom.  I'm asking this because the one place predicate symbol Empty() is actually new, it is not among the listed primitives of say Zermelo, which has only identity and membership as primitive symbols.   So when we are stating definitions are we in effect stating axioms? but instead of being about characterizing a primitive, they are definitional axioms giving a complete reference to a specified set of symbols in the system.  Is that correct?  Now if that is the case, then why we don't call it axiom when we state it, I mean why we don't say for example:  Definitional axiom 1) $Empty(x) \iff \not \exists y (y \in x)$  Zuhair 
A.33	Physical meaning and significance of third derivative of a function. Given a physical quantity represented by a function $f(t,x)$ what is (if there is any) the actual meaning of the third derivative of $f$, $\frac{\partial^3 f}{\partial t^3}$ or $\frac{\partial^3 f}{\partial x^3}$ 
A.34	Extending Knuth up-arrow/hyperoperations to non-positive values. So... I had the silly idea to extend Knuth's up-arrow notation so that it included zero and negative arrows. It is normally defined as $$\begin{align*} a \uparrow b &amp; = a^b \\ a \uparrow^n b &amp; = \underbrace{a \uparrow^{n - 1} (a \uparrow^{n - 1} (\dots(a \uparrow^{n - 1} a) \dots ))}_{b\text{ copies of } a} \end{align*}$$ so, basically the hyperoperation sequence starting from exponentiation. For now, I will only consider $a,b &gt; 0$.  If we try to go backwards from $a \uparrow b$, the "trivial" extension (letting down arrows represent negative up arrows, because why the heck not) is: $$\begin{align*} a \;b &amp; = a \cdot b \\ a \downarrow b &amp; = a + b \\ a \downarrow \downarrow b &amp; = \text{see below} \end{align*} \\ \vdots$$ But I had trouble coming up with an expression for $a \downarrow \downarrow \downarrow b$.  Maybe it doesn't exist. Alternatively, maybe there is a way of defining $a \; b$ (zero arrows) such that it does exist. So my question is: Is there an extension of Knuth's up-arrow notation such that $a \downarrow^n b$ exists for all $n \geq 3$?  Edit: Welp, I messed this question up. I initially thought $a \downarrow \downarrow b = a + 1$ was correct, but it is actually $b + 1$. So I thought I had an example of an extension when I did not. I have modified the question accordingly.  An extension would define $a \uparrow^n b$ for each $n \leq 0$ which satisfies the recursive definition of the notation.    Edit 2: Okay, turns out $a \downarrow \downarrow b = b + 1$ isn't correct either, as this would imply $a \downarrow b = a + b - 1$. For example, $4 \downarrow 3 = 4 \downarrow \downarrow (4 \downarrow \downarrow 4) = 4 \downarrow \downarrow (4 + 1) = (4 + 1) + 1 = 6 = 4 + 3 - 1$. But it is really close; perhaps we need an exception, such as $$\begin{align*} a \downarrow \downarrow b = \begin{cases}b + 1 &amp; \text{if } a &lt; b \\ b + 2 &amp; \text{if } a = b \end{cases}\end{align*}.$$ The case $a &gt; b$ does not show up when evaluating $a \downarrow b$, but it will be need to be defined if we try to extend further to $a \downarrow \downarrow \downarrow b$. For instance, we could abuse the fact that the case $a &gt; b$ is allowed to be anything, and let $$a \downarrow \downarrow b = b + 1 + \left\lfloor \frac{a}{b} \right\rfloor,$$ but finding $a \downarrow \downarrow \downarrow b$ may be intractable as a result. 
A.35	When does a function NOT have an antiderivative?. I know this question may sound naïve but why can't we write $\int e^{x^2} dx$ as $\int e^{2x} dx$? The former does not have an antiderivative, while the latter has.  In light of this question, what are sufficient conditions for a function NOT to have an antiderivative. That is, do we need careful examination of a function to say it does not have an antiderivative or is there any way that once you see the function, you can right away say it does not have an antiderivative? 
A.36	Proof by contradiction, status of initial assumption after the proof is complete.. First of all I'd like to say that I have looked for the answers to my specific question and have not found it in the existing topics.  The question is fairly simple. Say, we need to  prove statement P by the method of contradiction. Assuming that $\lnot P$ holds, using the list of statements proven earlier to hold or derived by us during the proof, we arrive to P being $true$.  $$\lnot P  \to A_1 \to\ ... \ \to A_n \to P$$ $$\lnot P  \to  P \iff \lnot(\lnot P) \lor P \iff P $$  We can therefore add P to the list of our proven statements, because it was derived. Most of the proofs contain something in the lines of "the obtained contradiction proves that our initial assumption ($\lnot P$) was wrong and so $P$ holds".  What I don't understand is, if the initial assumption ($\lnot P$) is thus proven to be false, then why can we be sure that anything derived from it holds (in particular, that P holds)? On the other hand, if it cannot be derived then the assumption ($\lnot P$) can in fact be true.   Can someone explain why this type of argument cannot be used? 
A.37	Non trivial examples of $f\circ g = g \circ f$ but $f^{-1} \neq g$ and $f\neq\mathrm{id}\neq g$.. Are there real-valued functions $f$ and $g$ which are neither each other's inverses, the identity, nor linear, yet exhibit the behaviour $$f\circ g = g \circ f?$$  Examples such as $f(x) = 2x$ and $g(x)=3x$ are "trivial" in this sense.   Moreover, given a function $f$, can one go about obtaining an example of a function $g$ which commutes with $f$?   I suppose this would be similar to fixed point iteration? E.g. if $f\colon\mathbb R\smallsetminus\{1\}\to\mathbb R$ is defined by $f(x) = 2x/(1-x)$, I would need a function $g$ such that  $$g(x) = f^{-1}\circ g\circ f =\frac{g(\frac{2x}{1-x})}{2+g(\frac{2x}{1-x})},$$ so maybe choosing an appropriate "starting function" $g_0$ and finding a fixed point of $g_{n+1} \mapsto f^{-1}\circ g_n\circ f$ would be a possible strategy, but I can't seem to find a suitable $g_0$. 
A.38	Uses of Axiom of Choice. I am a first-year maths student but I occasionally drift away from our taught material. Some years ago I saw the ZFC axioms for the first time, but now that I am in college, and although the stuff I've been taught so far is nowhere near ZFC (in terms of difficulty), it happened to me that we use the axiom of choice all the time in every module, even if we don't know it by name yet.  For example, in the proof that, for every non-negative integers $a, b$, there exist integers $q, r: a = bq + r$ (with the known restrictions on r), and the proof starts like this: $Choose$ the largest integer $q : qb &lt;= a$... blah blah blah.  Is it the axiom of choice that allows us to execute this simple yet so important step?   And a couple more questions: Can you name some other simple proofs, theorems, results etc for which the axiom of choice is essential?  Also, I've read that AOC has long been a topic of dispute for mathematicians, and that even today, some people do not accept it. Are there any alternative axiomatic systems that work equally well without needing AOC? Thanks! 
A.39	How to know which value is bigger?. Which is bigger between $2018^{2019}$ or $\ 2019^{2018}\ $?  When taking logs of both sides and  I get:   $2019\log(2018)\ $ and $\ 2018 \log(2019)$  I know $\log 2019\gt \log 2018$ so does this mean that $2019^{2018}$ is the biggest one? And did I do it properly? 
A.40	What is the meaning of the term "linear". $a_1x_1+a_2x_2+a_3x_3+...+a_nx_n=$ is called a linear equation because it represents the equation of a line in an n dimensional space. So "linear" comes from the word "line".Basically there should not be any higher power of x failing which the graph of the function will not be a straight line.  simillarly  $a(x)y+b(x)y'+c(x)y"+d(x)y'''+...+q(x)=0$ is also called linear differential equation because all the derivatives have power equal to 1 which is similar to the above definition of a linear equation.  A function f is called linear if: $f(x+y)=f(x)+f(y)$ and $f(cx)=cf(x)$. Here c is a constant. In this definition of linearity of function "$f$" what does the word linear means? How does it relate to a straight line?  Finally what does the term linear means in case of linear vector spaces? Where is the reference to a straight line?  So, whether linear is just a word used in different contexts? Does it have different meaning in different situation? Or linearity refers to some relation to a straight line? At Least please explain how the linearity of function f and linear vector space relate to the equation of a line. 
A.41	Confusion in how to find number of onto functions if two sets are given. In the book it is given if A and B are two finite sets containing $m$ and $n$ elements, respectively, then the number of onto functions from A to B will be if   $n \leq m$   $$\sum_{r=1}^n (-1)^{(n-r)} {n \choose r}(r)^m $$  well I can't understand it and I am aware with combinations. 
A.42	What is a simple, physical situation where complex numbers emerge naturally?. I'm trying to teach middle schoolers about the emergence of complex numbers and I want to motivate this organically. By this, I mean some sort of real world problem that people were trying to solve that led them to realize that we needed to extend the real numbers to the complex.   For instance, the Greeks were forced to recognize irrational numbers not for pure mathematical reasons, but because the length of the diagonal of a square with unit length really is irrational, and this is the kind of geometrical situation they were already dealing with. What similar situation would lead to complex numbers in terms that kids could appreciate?  I could just say, try to solve the equation $x^2 + 1 = 0$, but that's not something from the physical world. I could also give an abstract sort of answer, like that $\sqrt{-1}$ is just an object that we define to have certain properties that turn out to be consistent and important, but I think that won't be entirely satisfying to kids either. 
A.43	Prove $\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$. I am trying to prove  $$\sum_{n\geq1}\frac1{n^2+1}=\frac{\pi\coth\pi-1}2$$ Letting $$S=\sum_{n\geq1}\frac1{n^2+1}$$ we recall the Fourier series for the exponential function $$e^x=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos nx-n\sin nx)$$ Plugging in $x=\pi$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}(\cos n\pi-n\sin n\pi)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi\sum_{n\geq1}\frac{(-1)^n}{n^2+1}((-1)^n-n\cdot0)$$ $$e^\pi=\frac{\sinh\pi}\pi+\frac{2\sinh\pi}\pi S$$ $$S=\frac{\pi e^\pi}{2\sinh\pi}-\frac12$$ But that is nowhere near to correct. What did I do wrong, and how do can I prove the identity? Thanks. 
A.44	For $A,B \in \mathscr{M}_{2\times2}(\mathbb{Q}) $ of finite order, show that $AB$ has infinite order. Let $G$ be the group $ ( \mathscr{M}_{2\times2}(\mathbb{Q}) , \times ) $ of nonsingular matrices.  Let $ A = \left ( \begin{matrix}  0 &amp; -1 \\   1 &amp; 0  \end{matrix} \right ) $, the order of $A$ is $4$;  Let $ B = \left ( \begin{matrix}  0 &amp; 1 \\   -1 &amp; -1  \end{matrix} \right ) $, the order of $B$ is $3$.  Show that $AB$ has infinite order.    The only reasoning possible here is by contradiction as $G$ is not abelian. And so I tried, but I got stuck before any concrete development.  Any hints are welcome, Thanks. 
A.45	How to prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$?.    Prove that {$\sin(x) , \sin(2x) , \sin(3x) ,...,\sin(nx)$} is independent in $\mathbb{R}$   my trial :  we know that the Wronsekian shouldn't be $0$ to get the trivial solution and thus they are independent. its not trivial to show that $ W \not = 0$  W =   $\begin{vmatrix} (1)\sin(x) &amp; (1)\sin(2x) &amp; (1)\sin(3x) &amp;  ... &amp;   (1)\sin(nx) \\ (1)\cos(x) &amp; (2)\cos(2x) &amp; (3)\cos(3x) &amp;  ... &amp;   (n)\cos(nx) \\  -(1)^2\sin(x) &amp; -(2)^2\sin(2x) &amp; -(3)^2\sin(3x) &amp;  ... &amp;   -(n)^2\sin(nx) \\ -(1)^3\cos(x) &amp; -(2)^3\cos(2x) &amp; -(3)^3\cos(3x) &amp;  ... &amp;   -(n)^3\cos(nx) \\ \end{vmatrix}$  and so on. it looks like Vandermonde matrix but i cant prove that and so we conclude that its $W\not =0$ 
A.46	Suppose $f$ is a Lebesgue integrable function on$[0,1]$ which satisfies $\int x^k f(x) dx=0$ , prove $f=0 \text{ a.e.}$. Suppose $f$ is an indefinitely differentiable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\, dx=0$ for $k=\{0,1,2,3,.. .\}$, prove $f=0$ . My attempt : To prove this assertion , it suffice to prove $$\int_0^1 f^2 (x) \,dx=0$$  Then by approximation theorem , we can find a polynomial such that for every $\epsilon \gt 0$ $$\int_0^1 f^2 (x) \,dx= \int_0^1 (f(x)-\sum_{n=0}^N a_n x^n)f(x) ,dx+\int_0^1 \sum_{n=0}^N f(x) a_n x^n\,dx \le \epsilon M$$ where $M=\sup_{x \in [0,1]}|f(x)|$ . It seems that we do not need the condition that $f$ is indefinitely differentiable , if $f$ is continuous then the conclution may hold . My question : a) Suppose $f$ is a Lebesgue integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?       b) If a) is not true , suppose $f$ is a Riemann integrable real valued function on$[0,1]$ which satisfies $\int_0^1 x^k f(x)\ dx=0$ for $k=\{0,1,2,3,.. .\}$, can we prove $f=0$ except on a set of measure $0$ ?    EDIT: To prove $f=0 \text{  a.e.}$ , it suffice to prove all the fourier coefficient of $f$ equal to $0$ , then  $$\int_0^1 f(x) \cos(2 \pi nx) \,dx \le \int_0^1 |f(x)||\cos(2 \pi nx)-\sum_{n=0}^{N}a_n x^n| \le \epsilon||f||_{L^1}$$ and the proof is complete. 
A.47	Prove that for a given prime $p$ and each $0 < r < p-1$, there exists a $q$ such that $rq \equiv 1 \bmod p$. Prove that for a given prime $p$ and each $0 &lt; r &lt; p-1$, there exists a $q$ such that   $$rq \equiv 1 \bmod p$$  I've only taken one intro number theory course (years ago), and this just popped up in a computer science class (homework). I was assuming that this proof would be elementary since my current class in an algorithm cours, but after the few basic attempts I've tried it didn't look promising. Here's a couple approaches I thought of:    (reverse engineer)  To arrive at the conclusion we would need  $$rq - 1 = kp$$  for some $k$. A little manipulation:  $$qr - kp = 1$$  That looks familiar, but I can't see anything from it.    (sum on $r$)  $$\sum_{r=1}^{p-2} r = \frac{(p-2)(p-1)}{2} = p\frac{p - 3}{2} + 1 \equiv 1 \bmod p$$  which looks good but I don't know how to incorporate $r$ int0 the final equality.      (Wilson's Theorem—proved by Lagrange)    I vaguely recall this theorem, but I was looking at it in an old book and it wasn't easy to see how we arrived there. Anyways, $p$ is prime iff $$(p-1)! \equiv -1 \bmod p$$  Here the $r$ multiplier is built in to the factorial expression so I was thinking of adding $2$ to either side  $$(p-1)! + 2 \equiv 1 \bmod p$$  which is a dead end (pretty sure). But then I was thinking, maybe multiplying Wilson't Thm by $(p+1)$? Then getting  $$(p+1)(p-1)! = -(p+1) \bmod p$$  which I think results in  $$(p+1)(p-1)! = 1 \bmod p$$  of which $r$ is a multiple and $q$ is obvious. But I'm not sure if that's valid. 
A.48	Hints for showing that if $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \geq 1$. I'm trying to show the following:          If $x,y \geq 0$, then $(x+y)^k \geq x^k + y^k$ for all $k \in \mathbb{R_{\geq 1}}$      So, far I've tried a few things, but nothing seems to stick. Clearly $x \leq x+ y$ and since both sides of the inequality are positive, the inequality $x^k \leq (x + y)^k$ will hold for $k \geq 1$. Similarly, $y^k \leq (x+y)^k$. Adding both inequalities together, we obtain: $x^k + y^k \leq 2(x+y)^k$. While this is close, of course, it is not what we want to show.  Alternatively, I was thinking that if we fix $x,y \geq 0$, let $f(k) = (x+y)^k$, and let $g(k) = x^k + y^k$, we can show using the binomial theorem that $(x+y)^r \geq x^r + y^r$ for all $r \in \mathbb{Z^+}$. Then, if we can show both $f$ and $g$ intersect only when $k =1 $, we might have better luck proving the statement since then we would have $(x+y)^r &gt; x^r + y^r$ for all positive integers $r \geq 2$. A real number $m \notin \mathbb{Z}$ with the property that $f(m) = g(m)$ could not then exist since then that would violate the fact that $k=1$ is the only intersection. We could then invoke the continuity of $f$ and $g$ together with the fact that $(x+y)^r &gt; x^r + y^r$ for all positive integers $r \geq 2$ to obtain our result. Of course, all of this is dependent on rigorously showing that $f$ and $g$ intersect only when $k=1$.  Otherwise, I'm running low on ideas. Any hints would be greatly appreciated. 
A.49	Is there a simple combinatoric interpretation of this identity?. I came across an exercise in which we are asked to prove the identity:  $${2n\choose n}=\sum_{k=0}^n{n\choose k}^2$$  The exercise gives the hint:  $$\left(1+x\right)^{2n}=\left[(1+x)^n\right]^2$$  It's not too difficult to use the hint to prove the identity (the expressions in the identity are the coefficients of $x^n$ in the respective expansions of the expressions in the hint, which of course must be the same number), but I was wondering whether there is a neater equivalent-counting interpretation...  It's clear that ${2n \choose n}$ is the number of ways in which we can choose half the elements in a set (where this is possible): how can we interpret $\sum_{k=0}^n{n\choose k}^2$ equivalently? 
A.50	Divergent series $\sum{\frac{1}{n^{2+\cos{n}}}}$. Bonjour.  Show that  $$\sum{\frac{1}{n^{2+\cos{n}}}}$$ is a divergent serie. $$\\$$  My main problem is: If $\epsilon$ is “infinitely small positive real number” define $A_{\epsilon}$ as the set of all $n, |2+\cos n|\leq 1+\epsilon$ $(n \in A_{\epsilon}\iff-1\leq \cos n \leq -1+\epsilon)$. The divergence should come from the sum over $A_{\epsilon}$ but I have no idea to how to handle this. $$\\$$ 
A.51	Sum of series having binomial coefficients. Prove that $\displaystyle \sum_{r=0}^n {n+r\choose r} \frac{1}{2^{r}}= 2^{n}$  what i try  $$\binom{n}{n}+\binom{n+1}{1}\frac{1}{2}+\binom{n+2}{2}\frac{1}{2^2}+\binom{n+3}{3}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}$$  $$\binom{n}{n}+\binom{n+1}{n}\frac{1}{2}+\binom{n+2}{n}\frac{1}{2^2}+\binom{n+3}{n}\frac{1}{2^3}+\cdots +\binom{n+n}{n}\frac{1}{2^n}.$$  coefficient of $x^n$ in   $$(1+x)^n+(1+x)^{n+1}\frac{1}{2}+(1+x)^{n+2}\frac{1}{2^2}+\cdots\cdots +(1+x)^{2n}\frac{1}{2^n}.$$  How do i solve ithelp me plesse 
A.52	Prove $\forall n\in\mathbb{N}$, $\exists m\in\mathbb{N}$ s.t. $m>n$ and $m$ is prime. There are two parts I am having trouble getting started.  A. Prove that $n_1, n_2,...,n_k\in\mathbb{N}$ are each at least $2$ then $n=n_1n_2...n_k+1$ is not divisible by any numbers $n_1, n_2,...,n_k$.   B. Prove that the truth of the negation leads to a contradiction. (Use theorem: For all $a,b\in\mathbb{N}$ there exist a unique quotient $q$ and remainder $r$ in $\mathbb{Z^+}$ such that we have both $a=qb+r$ and $0\leq r&lt;q$.)  For part A, I started with, given $k\in\mathbb{N}$ and $n_1, n_2,...,n_k\geq1$, I'll show that $\forall i$, $n_i \nmid n=n_1n_2...n_k+1$ to set it up, but I'm not sure how to actually go about starting it.  For part B, I know that the negation is $\exists n\in\mathbb{N}$ s.t. $\forall m\in\mathbb{N}$ either $m\leq n$ or $m$ is not prime, but again I'm not sure what I should do to start the proof or exactly how to incorporate that theorem. 
A.53	Show that one-sided inverse of a square matrix is a true inverse. We know that for a group element $g\in G$, $gh=1$ does not necessarily mean that $hg = 1$. In the case for matrices (linear maps between vector spaces), it is also true that $AB = 1 \nRightarrow BA = 1$. This happens when the $A$ and $B$ are not square matrices (in which case they do not even form a group under multiplication).  However if we restrict the square matrices, $AB = 1 \Rightarrow BA = 1$. What is simple proof of this that avoids chasing the entries, and makes use simply the vector space structure of linear transformations?  (In fact if we could prove this, I think this might imply that for a group to have one-sided(but not two-sided) inverses, it has to be infinite, since every finite group admits a finite dimensional representation. 
A.54	By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable.. Any tips or solutions for this one?  By using a diagonal argument, show that the powerset $P(N) = (S|S ⊆ N)$ is uncountable. 
A.55	$\frac{1}{\sqrt{-1}}=\sqrt{-1}$?. I have trouble to comprehend what my mistake is in the following calculation:  If we set $\sqrt{-1}$ to be the new number with the property that $(\sqrt{-1})^2 = -1$ then I can write $$\frac{1}{\sqrt{-1}}=\sqrt{\frac{1}{-1}}=\sqrt{-1}.$$  But we also have (and I know this is the correct result) $$ \frac{1}{\sqrt{-1}}\cdot\frac{\sqrt{-1}}{\sqrt{-1}}=\frac{\sqrt{-1}}{-1}=-\sqrt{-1}$$  What am I missing? Thanks. 
A.56	A curious logical formula involving prime numbers. Let $S$ be a nonempty set of natural numbers. Is the following formula $$ \exists p\ \bigl(\text{$p$ is prime } \rightarrow \forall x  \text{ ($x$ is prime)}\bigr)  $$ true or false on $S$? I know the answer to this question, but what would be the shortest way to arrive to the conclusion using some deduction system? 
A.57	Preimage of continuous one-to-one function on connected domain is not continuous.. I know that given $B$, a compact subset of $\mathbb{R}^n$, and $f : B \to \mathbb{R}^m$, a continuous injective (one-to-one) function, $f^{-1}$ is continuous on $f(B)$. (This true).  I also know that image $f(X)$ of a connected subset $X$ is connected under a continuous function.  Now let $X$ be a connected (non-compact) subset of $\mathbb{R}^n$, and $f : X \to \mathbb{R}^m$ be a continuous injective (one-to-one) function. I am trying (and struggling) to provide a counterexample in which mapping $f^{-1} : f(X) \mapsto X$ is not continuous on $f(X)$. (A rigorous, parametrized example).  Thank you in advance! 
A.58	Prove that $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$. Can someone help me with this exercise? I honestly don't know where to start and how to prove it. You don't have to answer it fully, just give me a hint or something. Thank you in advance.     Exercise 1. Prove that  $3\arcsin \frac{1}{4} + \arccos \frac {11}{16} = \frac {\pi}{2}$   Thanks. 
A.59	Multiple proofs of $\sum_{d|n}{\phi(d)}=n$. I am looking for multiple proofs of that statement: here $\phi(n)$ denotes the Euler’s totient  $$\sum_{d|n}{\phi(d)}=n$$   Here’s one:   By unique factorisation theorem: $n=\prod_{k=1}^{m}{p_k^{\alpha_k}}$ and $d=\prod_{k=1}^{m}{p_k^{\beta_k}}$ where $0\leq \beta_k\leq \alpha_k$ so:   $\begin{align} \sum_{d|n}{\phi(d)}&amp;=\sum_{0\leq \beta_k\leq \alpha_k}{\phi\left(\prod_{k=1}^{m}{p_k^{\beta_k}}\right)}\\ &amp;= \sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}\phi({p_k^{\beta_k})}}\\ &amp;=\sum_{0\leq \beta_k\leq \alpha_k}{\prod_{k=1}^{m}{(p_k^{\beta_k}-p_k^{\beta_k-1}})}\\ &amp;=\prod_{k=1}^{m}{\sum_{0\leq \beta_k\leq \alpha_k}{(p_k^{\beta_k}-p_k^{\beta_k-1}}})\\ &amp;= \prod_{k=1}^{m}{p_k^{\alpha_k}}\\ &amp;=n. \end{align}$ 
A.60	Limiting value of a sequence when n tends to infinity. Q) Let, $a_{n} \;=\; \left ( 1-\frac{1}{\sqrt{2}} \right ) ... \left ( 1- \frac{1}{\sqrt{n+1}} \right )$ , $n \geq 1$. Then $\lim_{n\rightarrow \infty } a_{n}$  (A) equals $1$  (B) does not exist  (C) equals $\frac{1}{\sqrt{\pi }}$  (D) equals $0$    My Approach :- I am not getting a particular direction or any procedure to simplify $a_{n}$ and find its value when n tends to infinity. So, I tried like this simple way to substitute values and trying to find the limiting value :- $\left ( 1-\frac{1}{\sqrt{1+1}} \right ) * \left ( 1-\frac{1}{\sqrt{2+1}} \right )*\left ( 1-\frac{1}{\sqrt{3+1}} \right )*\left ( 1-\frac{1}{\sqrt{4+1}} \right )*\left ( 1-\frac{1}{\sqrt{5+1}} \right )*\left ( 1-\frac{1}{\sqrt{6+1}} \right )*\left ( 1-\frac{1}{\sqrt{7+1}} \right )*\left ( 1-\frac{1}{\sqrt{8+1}} \right )*.........*\left ( 1-\frac{1}{\sqrt{n+1}}    \right )$     =$(0.293)*(0.423)*(0.5)*(0.553)*(0.622)*(0.647)*(0.667)* ....$ =0.009*...  So, here value is tending to zero. I think option $(D)$ is correct. I have tried like this $\left ( \frac{\sqrt{2}-1}{\sqrt{2}} \right )*\left ( \frac{\sqrt{3}-1}{\sqrt{3}} \right )*\left ( \frac{\sqrt{4}-1}{\sqrt{4}} \right )*.......\left ( \frac{\sqrt{(n+1)}-1}{\sqrt{n+1}} \right )$ = $\left ( \frac{(\sqrt{2}-1)*(\sqrt{3}-1)*(\sqrt{4}-1)*.......*(\sqrt{n+1}-1)}{{\sqrt{(n+1)!}}} \right )$ Now, again I stuck how to simplify further and find the value for which $a_{n}$ converges when $n$ tends to infinity . Please help if there is any procedure to solve this question.  
A.61	There exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$.    Prove that there exists $i, j \in \mathbb{N}$ such that $n=3i+5j$ for $n\ge 8$   I'm having a hard time with this exercise, I'm trying to prove it by induction:  Basis step:   $n=8 \implies 8=3\cdot1+5\cdot 1$ $n=9 \implies 9=3\cdot3+5\cdot0$ $n=10 \implies 10=3\cdot0+5\cdot2$   Induction step:  If it's true for $n=h$ then it must be true for $n=h+1$.  So now, I don't know how to begin proving that $k+1=3i+5j$. 
A.62	Prove that the cardinality of the set of rational numbers and the set of integers is equal. I just learned about cardinality in my discrete class a few days ago, and this is in the homework. This is all fairly confusing to me, and I'm not entirely sure where to even start. Here's the full question:  Let $\mathbb{Q}$ denote the set of rational numbers and $\mathbb{Z}$ denote the set of integers. Prove that $|\mathbb{Q}| = |\mathbb{Z}|$.  I thought about saying that every element in $\mathbb{Q}$ can be written as some element in $\mathbb{Z} \times \mathbb{Z}$, but I still don't know how to prove that that is a bijection, or even how to prove that $|\mathbb{Z} \times \mathbb{Z}| = |\mathbb{Z}|$.  Any help would be greatly appreciated. 
A.63	$\gcd$ and $\text{lcm}$ of more than $2$ positive integers. For any two positive integers ${n_1,n_2}$, the relationship between their greatest common divisor and their least common multiple is given by  $$\text{lcm}(n_1,n_2)=\frac{n_1 n_2}{\gcd(n_1,n_2)}$$  If I have a set of $r$ positive integers ${n_1,n_2,n_3,...,n_r}$, does the same relationship hold? Is it true that  $$\text{lcm}(n_1,n_2,n_3,...,n_r)=\frac{\prod_{i=1}^r n_i}{gcd(n_1,n2,n_3,...,n_r)}$$  I feel like this should be easy to prove, but I'm struggling to get a handle on it. 
A.64	Suppose that f : [a, b] → R is continuous and that f([a, b]) ⊂ [a, b]. Prove that there exists a point c ∈ [a, b] satisfying f(c) = c.. Suppose that $f : [a, b] \to \mathbb{R}$ is continuous and that $f([a, b]) \subset [a, b]$. Prove that there exists a point $c \in [a, b]$ satisfying $f(c) = c$.   (If either $f(a) = a$ or $f(b) = b$ there is nothing left to show, so you might as well assume that $f(a) = a$ and $f(b) = b$. Since $f$ takes its values in $[a, b]$ this is the same as assuming that $f(a) &gt; a$ and $f(b) &lt; b$.)  So far, I have:  Pf. Assume $f(a)&gt;a$ and $f(b)&lt; b$.  Let $x, y \in [a,b]$ such that $f(a)=x$ and $f(b)=y$ which means $f[a,b]=[x,y]$. Notice $[x,y]\subset [a, b]$. Since f is continuous on $[x,y]$, there exists some $c \in [a,b]$ such that $x$ is less than or equal to $c$ is less than or equal to $y$...  This is where I am stuck because I don't think I can just assume by Intermediate Value Theorem that some $f(c)=c$? 
A.65	How can we show that $e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}$ for all $\lambda,t\ge0$?. How can we show that $$e^{-2\lambda t}\lambda^2\le\frac1{e^2t^2}\tag1$$ for all $\lambda,t\ge0$?  Applying $\ln$ to both sides yields that $(1)$ should be equivalent to $$t\lambda\le e^{t\lambda-1}\tag2.$$ So, if I did no mistake, it should suffice to show $x\le e^{x-1}$ for all $x\ge0$. How can we do this? 
A.66	if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$ is it possible to justify that $(x^TAh)^T = h^TA^Tx$?. if $x,h \in \mathbb{R}^d$ and $A \in \mathbb{R}^{d\times d}$  is it possible to justify that $(x^TAh)^T = h^TA^Tx$? 
A.67	Combination of matrixes. If A is a $k\times k$ matrix,B is a $k\times l$ matrix and C is a $l\times l$ matrix prove that:  $\det{\begin{bmatrix}A&amp;B\\O&amp;C\end{bmatrix}}=\det(A)\det(C)$  O is the matrix that all it's elements are equal to zero.  I know some rules for calculating determinants but I don't know how to begin in this question. 
A.68	Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd. Prove $a^n+1$ is divisible by $a + 1$ if $n$ is odd:  We know $a$ cannot be $-1$ and the $n \in \mathbb{N}$. Since $n$ must be odd, we can rewrite $n$ as $2k+1$. Now we assume it holds for prove that it holds for the next term.  $$a^{2(k+1)+1}+1$$ $$=a^{2k+3}+1$$ $$=a^3\cdot a^{2k}+1$$ $$=(a^3+1)\cdot a^{2k} -a^{2k}+1$$  Im not sure on what to do next. Since $a^{2k}$ means that the exponential term will be even and thus you cant use the fact that $a^n+1$ is divisible by $a + 1$ if $n$ is odd.     
A.69	Induction with two variable parameters. So I was assigned this homework problem: $$\ {s \choose s} + {s+1 \choose s} +...+ {n \choose s} = {n+1 \choose s+1}$$ for all s and all $n \geq s$ I've tried to email both my professor and my TA and their explanations seem contradictory. My professor responded saying the statement I need to prove is "The formula is correct for $0 \leq s \leq n$." Whereas my TA told me I need to use induction on both variables and I'm not sure how to do that. Any help is appreciated! 
A.70	Proving $\sum_{j=0}^{N-1}\cos\frac{\left(2j+1\right)\pi}{2N}=0$. Let $l\in\mathbb{Z}$ and $N\in\mathbb{N}$. I need to prove the following: $\begin{equation} \sum_{j=0}^{N-1}\cos\left(l\frac{\left(2j+1\right)\pi}{2N} \right)=0 \end{equation}$ I tried to use Euler formula and then sum the first $N$ terms of the geometric serie I get, but it didn't work. Any ideas? 
A.71	Show, with induction that $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$. Show, with induction that  $1^2 + 2^2 + .... + n^2 = \frac{n(n+1)(2n+1)}{6}$  My attempt  Case 1: n = 1  $LHS = 1^2$   $RHS = \frac{(1+1)(2+1)}{6} = \frac{2*3}{6} = 1$  Case 2: n = p  $LHS_{p} = 1^2 + 2^2 + ... + p^2$  $RHS_{p} = \frac{p(p+1)(2p+1)}{6}$  Case 3: n = p + 1  $LHS_{p+1} = 1^2+2^2+....+p^2+(p+1)^2$  $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6}$  Now to show this with induction I think i need to show that  $RHS_{p+1} = RHS_{p} + (p+1)^2$  $RHS_{p+1} = \frac{p(p+1)(2p+1)}{6} + (p+1)^2$  So I need to rewrite   $RHS_{p+1} = \frac{(p+1)((p+1)+1)(2(p+1)+1)}{6} $to be equal to $\frac{p(p+1)(2p+1)}{6} + (p+1)^2$  Anyone see how I can do that? Or got any other solution? 
A.72	Is it possible that $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$?. Is it possible for a set to equal another set, yet the former set be an element in the latter set?  I.e.:  $\mathcal{X} = \mathcal{Y}$, yet $\mathcal{X} \in \mathcal{Y}$ 
A.73	Help on proof of $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$. The proof is required to be made through the binomial theorem. I will expose the demonstration I was tought, and forward my questions after exposing it. You'll see question marks like this one (?-n) on points I don't quite understand, where $n$ is the numeration of the mark. This are the doubts I have about the demonstration, the which I hope someone can clarify.  Prove that $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$.  We will use the following equality, and call it $P$:   $(1+x)^n(1+x)^n=(1+x)^{2n}$    (?-1)  The result will be proved finding the $x^n$ coefficient of both terms of this equality (?-2).  According to the binomial theorem, the left-hand side of this equation is the product of two factors, both equal to  $\binom{n}{0}1+\binom{n}{1}x+...+\binom{n}{r}x^r+...+\binom{n}{n}x^n$  When both factors multiply, a term on $x^n$ is obtained when a term of the first factor has some $x^i$ and the term of the second factor has some $x^{n-i}$. Therefor the coefficients of $x^n$ are  $\binom{n}{0}\binom{n}{n}+\binom{n}{1}\binom{n}{n-1}+\binom{n}{2}\binom{n}{n-2}+...\binom{n}{n}\binom{n}{0}$ .  Since $\binom{n}{n-r}=\binom{n}{r}$, the previous summation is equal to $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2$. So the left hand side of the equation we are asked to proove is a coefficient of $x^n$. When we expand the right-hand side of the equation $P$, we find that $\binom{2n}{n}$ is a coefficient of $x^n$. Therefore (?-3) the left-hand side of the equation we were asked to prove is in deed equal to $\binom{2n}{n}$. In conclussion,  $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$.  This was all the demonstration. My doubt one (?-1) goes about where the heck does this equation come from? How would I know what equation to come up with if requested to prove a different equality?  Doubt two (?-2) goes about why would the solution of the first equation would have anything to do with finding the $x^n$ coefficients of the one I just made up (see doubt one).  Doubt three (?-3) goes about why demonstrating that $a$ is a coefficient of $x^n$ on the left hand side of the equation I made up, and that $b$ is a coefficient of $x^n$ on the right-hand side of this equation as well, would prove my original equation, the one I was supposed to prove on the first place?  I know there are many doubts here, I hope you guys can help me. Sorry for the long post, it's a long demonstration. 
A.74	Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$..  Show that the image of the function $f:(0,\infty)\rightarrow \mathbb{R}$, $f(x)=x+\dfrac{1}{x}$ is the interval $[2,\infty)$.   If $x=1$, then $f(1)=2$. So how can I show that the mage of the function is the interval $[2,\infty)$? 
A.75	Prove that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $. I'm unsure how to show  that for each integer $m$, $ \lim_{u\to \infty} \frac{u^m}{e^u} = 0 $.   Looking at the solutions it starts with $e^u$ $&gt;$ $\frac{u^{m+1}}{(m+1)!}$ but not sure how this is a logical step. 
A.76	Covering $\mathbb{Z}$ by arithmetic progressions. I am solving problems from an old exam (in topology, but I've translated the problem into more algebraic terms). The problem is the following:     Let $a+b\mathbb{Z}=\{z\in \mathbb{Z}\mid z = a+bk \text{ for some  }k\in \mathbb{Z}\}$ where $a\in \mathbb{Z}$ and $b\in  \mathbb{Z}-\{0\}$. Suppose we have a collection of such sets   $\{a_i+b_i\mathbb{Z}\mid i \in \mathbb{N}\}$   satisfying:      $$\bigcup_{i \in \mathbb{N}}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$      Show whether it is always possible to extract a finite   $I\subset \mathbb{N}$ s.t.      $$\bigcup_{i \in I}(a_i+b_i\mathbb{Z})=\mathbb{Z}$$   Unfortunately, I seem to have forgotten a lot of my elementary algebra... Nevertheless, I have attempted something:  Let $\{p_k\}=\{2,3,5,\dots\}$ be the set of primes. We can construct: $$\left(\bigcup_{k\in \mathbb{N}}(0+p_k\mathbb{Z})\right)\cup (-1+\ell_1 \mathbb{Z})\cup (1+\ell_2\mathbb{Z})=\mathbb{Z}$$  for some appropriate non-negative integers $\ell_1,\ell_2$. We could for instance pick $\ell_1=\ell_2=5$. Suppose there is a finite sub-collection $\{0+p_{k_j}\}$, $j=1,\dots,n$ s.t.   $$\left(\bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})\right)\cup (-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$$  Now, assume $p$ is some prime s.t. $p&gt;\max\{p_{k_1},\dots,p_{k_n}\}$, then clearly $p\notin \bigcup_{1\leq j\leq n}(0+p_{k_j}\mathbb{Z})$. But here I run into a problem. I want $p\notin(-1+5 \mathbb{Z})\cup (1+5\mathbb{Z})$. That is, I want $5\nmid p-1$ and $5\nmid p+1$. This is of course possible if $p$ is a prime with a $7$ as its last digit. However, this approach means I have to prove that there are infinitely many primes ending on a $7$, which seems like a silly thing to prove for a simple problem like this. Surely, there is a nicer way of solving this?  EDIT: I am particularily interested in a solution not relying on topology, and whether a solution like my attempted solution works. 
A.77	Show that the relation $(- 1) (- 1) = 1$ is a consequence of the distributive law.    Show that the relation $(- 1) (- 1) =  1$ is a consequence of the distributive law.   This question is the first problem from 'Number Theory for Beginners" by Andre Weil. I cannot get the point from where to begin. I tried using $1\cdot 1 = 1$ and $ 1\cdot x = x $, but couldn't get somewhere. Can you help me just with a hint? I would be willing to work up from there. 
A.79	Inequality with complex exponential. Rudin in Real and Complex Analysis uses this in a proof near the beginning of chapter 9:     $\displaystyle \left \vert{ \frac {e^{-ixu}-1}{u}}\right\vert \le \vert x \vert$ for all real $u \ne 0$   Why is this true?  Edit: I believe $x$ is real 
A.80	Why does this proof that the set of all finite subsets of N is a countable set not work for the set of all subsets of N?. I found this proof in a StackExchange thread and found it pretty understandable and simple:  "The other answers give some sort of formula, like you were trying to do.  But, the simplest way to see that the set of all finite subsets of $\mathbb{N}$ is countable is probably the following.  If you can list out the elements of a set, with one coming first, then the next, and so on, then that shows the set is countable.  There is an easy pattern to see here.  Just start out with the least elements.  $$\emptyset, \{1\}, \{2\}, \{1, 2\}, \{3\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\}, \{4\}, \ldots$$  In other words, first comes $\{1\}$, then comes $\{2\}$.  Each time you introduce a new integer, $n$, list all the subsets of $[n] = \{1, 2, \ldots, n\}$ that contain $n$ (the ones that don't contain $n$ have already showed up).  Therefore, all subsets of $[n]$ show up in the first $2^{n}$ elements of this sequence."  I understand how it applies for finite subsets of N, but I cant really pinpoint of why it would not apply to a set of all subsets of N. We could continue this scheme for ever, couldnt we?  I assume that I think in a wrong way about infinity but I am not quite sure. Any help is greatly appreciated! 
A.81	Any infinite set contains a countable subset. Why is my proof wrong? (Axiom of Choice). Let $M$ be an infinite set.       Proposition 1:   For any $n \in \mathbb{N}$, there exists an injection from $\{1, \cdots, n\}$ to $M$.       (1) Since $M \neq \emptyset$, there exists $x \in M$. Define $f(1)$ as $f(1) := x$. $f$ is an injection from $\{1\}$ to $M$. (2) Suppose that there exists an injection $f$ from $\{1, \cdots, n\}$ to $M$. Since $M$ is an infinite set, $M - \{f(1), \cdots, f(n)\}$ is not an empty set. So, there exists $x \in M - \{f(1), \cdots, f(n)\}$. Define $g(1), \cdots, g(n+1)$ as $g(1) := f(1), \cdots, g(n):=f(n)$ and $g(n+1) := x$. Obviously, $g$ is an injection from $\{1, \cdots, n+1\}$ to $M$.    Let $n_1$ be an arbitrary natural number. If I wanna calculate $h(n_1)$, then I get an injection $g$ from $\{1, \cdots, n_1\}$ to $M$ by Proposition 1. And I return $g(n_1)$ as the value of $h(n_1)$. And I store the pairs $(1, g(1)), \cdots, (n_1, g(n_1))$ to my database.    If I wanna calculate $h(n_2)$ for $n_2 \leq n_1$, then I search my database and I get the value $g(n_2)$ from my database and I return $g(n_2)$ as the value of $h(n_2)$.  If I wanna calculate $h(n_3)$ for $n_3 &gt; n_1$, then I add the pairs $(n_1 + 1, g(n_1+1)), \cdots, (n_3, g(n_3))$ to my database by Proposition 1 and I return $g(n_3)$ as the value of $h(n_3)$.    I can calculate $h(n)$ for any $n \in \mathbb{N}$.    From above, we get an injection $h : \mathbb{N} \to M$.    Why is my proof wrong?  By the way. Suppose that a man wanna know if I have an injection $h : \mathbb{N} \to M$ or not. Then how can the man know if I have  an injection $h : \mathbb{N} \to M$ or not? 
A.82	All definite integrals evaluate to 0 using periodic functions.. I know that my reasoning is incorrect, I just don't know where I went wrong. I did discuss this with my Maths teacher, and even she could not find what I did wrong.  Let us begin by assuming a function, $f(x)$ that is continuous and has an antiderivative in the interval $[0, 2\pi]$. Let $A$ be the area under the curve for $f(x)$ in the interval $[0, 2\pi]$  $A = \displaystyle \int_{0}^{2\pi}{f(x)\space\mathrm{d}x}$  Now there must exist a function, $g(x)$ such that:    $f(x) = g(x)\cdot \cos(x)$  Substituting the value of $f(x)$:  $A = \displaystyle\int_{0}^{2\pi}{g(x)\cdot \cos(x)\space\mathrm{d}x}$  Using t substitution: Let $t = \sin(x)$ Then: $\mathrm{d}t = \cos(x)\space\mathrm{d}x$ And:  $x = \arcsin(t)$  Changing the limits: $t = \sin(x)$ $0$ becomes $\sin(0) = 0$ $2\pi$ becomes $\sin(2\pi) = 0$  Substituting in the definite integral:  $A = \displaystyle \int_{0}^{0}{g(\arcsin(t))\space\mathrm{d}t}$  But Definite Integral where the lower and upper bounds are the same is $0$. So:  $A = 0$, which is not possible.  Thanks for the help. 
A.83	Is the sequence of sums of inverse of natural numbers bounded?. I'm reading through Spivak Ch.22 (Infinite Sequences) right now. He mentioned in the written portion that it's often not a trivial matter to determine the boundedness of sequences. With that in mind, he gave us a sequence to chew on before we learn more about boundedness. That sequence is:  $$1, 1+\frac{1}{2}, 1+\frac{1}{2}+\frac{1}{3}, 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}, . . .$$  I know that a sequence is bounded above if there is a number $M$ such that $a_n\leq M$ for all $n$. Any hints here? 
A.84	Is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$?. I've : $I=&lt;p,x&gt;$ is not a principal ideal in $Z[x]$ where p is prime. My question :   Is $I=&lt;p,x&gt;$ a principal ideal in $Z[x]$ where p is not a prime? More particularly, is the ideal generated by ${4,x}$ a principal ideal in $Z[x]$ ? 
A.85	Expected number of steps for a bug to reach position $N$. A bug starts at time $0$ at position $0$. At each step, the bug either moves to the right by $1$ step $(+1)$ with probability $1/2$, or returns to the origin with probability $1/2$. What is the expected number of steps for this bug to reach position $N$?  I tried to first find the possibility that this bug reaches $N$ as the number of steps goes to infinity. The recurrence equation I find is $$p_n = \frac{1}{2}p_{n-1}$$, where $p_n$ is the possibility for the bug starting at position $n$ to reach $N$. We also have the boundary condition $p_N = 1$. Then we see that $p_{N-1}=2$, and that $p_0 = 2^N$, which doesn't make sense at all because it is greater than $1$. I think I should sort out the value of probability first, and think about the number of expected steps later.  I'm sure there is something wrong with the recurrence equation, but what's wrong about it? 
A.86	Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left(2 ^ {n\log _{3}n}\right)?$. Problem: Is it true that $\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)=O\left( 2 ^ {n\log _{3}n}\right)?$  My start of solution: $$\sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{k}\end{array}\right)\leq \sum_{k=0}^{n}k\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq \frac{n\cdot(n+1)}{2}\cdot \left(\begin{array}{l}{n}\\{\lfloor \frac{n}{2}\rfloor}\end{array}\right)\leq n(n+1)! \leq nn^n \leq n^{n+1}$$  I think this upper bound is way too large and I can't seem to find a solution. 
A.87	Is it true that $\forall n \in \Bbb{N} : (\sum_{i=1}^{n} a_{i} ) (\sum_{i=1}^{n} \frac{1}{a_{i}} ) \ge n^2$ , if all $a_{i}$ are positive?.    If  $\forall i \in \Bbb{N}: a_{i} \in \Bbb{R}^+$ , is it true that $\forall n \in \Bbb{N} : \big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n}  \frac{1}{a_{i}}\big) \ge n^2$ ?   I have been able to prove that this holds for $n=1$ , $n=2$, and $n=3$ using the following lemma:     Lemma 1:  Let $a,b \in \Bbb{R}^+$. If $ab =1$ then $a+b \ge 2$   For example, the case for $n=3$ can be proven like this:  Let $a,b,c \in \Bbb{R}^+$. Then we have:  $(a+b+c)\big(\frac{1}{a} + \frac{1}{b} + \frac{1}{c}\big) = 1 + \frac{a}{b} + \frac{a}{c} + \frac{b}{a} + 1 + \frac{b}{c} + \frac{c}{a} + \frac{c}{b}  + 1 $  $= 3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) $  By lemma 1, $\big(\frac{a}{b}  + \frac{b}{a}\big) \ge 2$,  $ \big(\frac{a}{c} + \frac{c}{a}\big) \ge 2$ and  $\big(\frac{b}{c} + \frac{c}{b}\big) \ge 2$ , therefore:  $3 + \big(\frac{a}{b}  + \frac{b}{a}\big) + \big(\frac{a}{c} + \frac{c}{a}\big) + \big(\frac{b}{c} + \frac{c}{b}\big) \ge 3 + 2 + 2 +2 = 9 = 3^2 \ \blacksquare $  However I'm not sure the generalized version for all natural $n$ is true. I can't come up with a counterexample and when I try to prove it by induction I get stuck.  Here is my attempt:  Let $P(n)::\big(\sum_{i=1}^{n}a_{i}\big) \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big) \ge n^2$  Base case: $\big(\sum_{i=1}^{1}a_{i}\big) \big(\sum_{i=1}^{1} \frac{1}{a_{i}}\big) = a_{1} \frac{1}{a_{1}} = 1 = 1^2$ , so $P(1)$ is true.  Inductive hypothesis:  I assume $P(n)$ is true.  Inductive step:  $$\left(\sum_{i=1}^{n+1}a_{i}\right) \left(\sum_{i=1}^{n+1} \frac{1}{a_{i}}\right) = \left[\left(\sum_{i=1}^{n}a_{i}\right) + a_{n+1}\right] \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$  $$=\left(\sum_{i=1}^{n}a_{i}\right) \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right] + a_{n+1} \left[\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + \frac{1}{a_{n+1}}\right]$$  $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +a_{n+1} \frac{1}{a_{n+1}}$$  $$=\left(\sum_{i=1}^{n}a_{i}\right)\left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +\left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) +1 $$  $$\underbrace{\ge}_{IH} n^2 + \left(\sum_{i=1}^{n}a_{i}\right) \frac{1}{a_{n+1}} + a_{n+1} \left(\sum_{i=1}^{n} \frac{1}{a_{i}}\right) + 1$$  And here I don't know what to do with the $\big( \sum_{i=1}^{n}a_{i} \big) \frac{1}{a_{n+1}} + a_{n+1} \big(\sum_{i=1}^{n} \frac{1}{a_{i}}\big)$ term.  Is this inequality true? If it is, how can I prove it? If it isn't, can anyone show me a counterexample? 
A.88	Is the polynomial $x^4+10x^2+1$ reducible over $\mathbb{Z}[x]$?. Is the polynomial  $x^4+10x^2+1$  reducible over  $\mathbb{Z}[x]$? 
A.89	Parametrization of pythagorean-like equation. Is there any known complete parametrization of the Diophantine equation $$ A^{2} + B^{2} = C^{2} + D^{2} $$ where $A, B, C, D$ are (positive) rational numbers, or equivalently, integers? 
A.90	Question on the definition of an Inverse matrix. By definition, if $A$ is a $ n \times n $ matrix, an inverse of $A$ is an $ n \times n $ matrix $A^{-1}$ with the property that:  $$ A^{-1}A=\mathbb I_n \ \ \land \ \ AA^{-1}=\mathbb I_n \ \ \ \ (1)$$   where $ \mathbb I_n $ is the $ n \times n $ identity matrix.  Are there any cases where $ A^{-1}A=\mathbb I_n$ but $AA^{-1} \neq \mathbb I_n$ or the other way around (and thus making (1) a false statement) ?  
A.91	Continuous function that reaches each value on its range exactly 2 times.. Is there a continuous function from $\mathbb{R}\to\mathbb{R}$ that reaches all of its possible values (each value in it's range) exactly $2$ times (for example, $x^2$ would be perfect if it wasn't for $0$..). Also, the same question but $3$ times.  I'm almost certain that there aren't such functions but who knows haha maybe there are a bunch... 
A.92	Is there a principal maximal ideal in $\mathbb F_q[X,Y]$?.    Given an infinite field $K$, one can prove that any maximal ideal of $K[X,Y]$ can't be principal. In fact, every non-principal prime ideal is a maximal ideal, and can be generated by two polynomials.      I am wondering whether the same result holds in $\mathbb F_q[X,Y]$. Can we find a principal ideal $I = (P(X,Y))$ for some irreducible polynomial $P$ that is a maximal ideal ?     Such a polynomial $P$ must have positive degrees in both $X$ and $Y$. Indeed, given an irreducible polynomial $Q(X)$ in only one variable, the quotient  $$\mathbb F_q[X,Y]/(Q(X))\cong (\mathbb F_q[X]/(Q(X)))[Y]$$ is a ring of polynomials over a field and as such, can never be a field.    Moreover, such a polynomial $P$ must have the form  $$P(X,Y) = \sum_{i=0}^n P_i(X)Y^i$$ where $d\geq 1$, the $P_i$'s are polynomials in $X$ and $P_n$ is a nonzero polynomial that vanishes identically on $\mathbb F_q$, thus $P_n$ must be divisible by $\prod_{\alpha \in \mathbb F_q} (X-\alpha)$. Indeed, if there were some $\alpha \in \mathbb F_q$ such that $P_n(\alpha) \not = 0$, then the ideal $I:=(P,X-\alpha)$ would contain $(P)$ strictly. If $(P)$ were to be maximal, $I$ would be the whole of $\mathbb F_q[X,Y]$. Writing down the fact that $1\in I$ and evaluating $X=\alpha$ would leads us to the conclusion $n=\deg_Y(P)=0$, which is absurd.    This is all I could infer so far. With respect to the above, I tried looking at $P(X,Y) = (X^{p-1}-1)XY - 1$ or $P(X,Y) = (X^{p-1}-1)XY - X - 1$ in $\mathbb F_p[X,Y]$ for some prime number $p$ but I have trouble determining whether the quotient is a field or not.    Would somebody know the answer of the problem, and according to it, give a proof or a counter-example ? Thank you very much in advance. 
A.93	Characteristic Polynomial $AB =$ characteristic polynomial $ BA$?. Let $A,B$ matrix on $\mathbb{R}$ size $nxn$. How can I prove that $det(xI - AB) = det(xI - BA)$ if $A$ and $B$ are singular matrix 
A.94	Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection. Question: Are there $2^{\aleph_{0} }$ sets of natural numbers such that each two have finite intersection.  From what I've read about infinite families, I need to ignore those who have the properpty $P$.  Property $P$: the family is threadless, but whenever we take finitely many sets from the family, those sets have infinite intersection.  and probably find ones with property $T$.  Property $T$: the family is threadless, and it is an “almost-tower”: Whenever you pick two sets in the family, one of them is almost-contained in the other. - probably meaning that we can find such an intersection which is finite, because members are contained in each other.  Then I thought..  To find them, I should think of rational numbers instead of natural numbers, remembering that rational numbers can be paired up with natural ones, so solving this problem for families of rational numbers is the same as solving it for families of natural numbers. Now, I need to consider various ways of defining the real numbers.  And I'm stuck.. any help is appreciated. 
A.95	Length of convex paths and bounding $\sin x$. The problem:     Defining $\sin x$ as the leg $b$ of a right triangle with $\angle B=x$ (in radians) and hypotenuse $1$, prove that $$\lim_{x\to 0}\frac{\sin x}x=1$$   (The motivation is to find the derivative of $\sin x$ in a elementary, "pre-Taylor" and "pre-series" context).  I have seen many times a proof that it is based in the fact that the length of the arc $x$ satisfies $$\sin x&lt;x&lt;\tan x$$ or sometimes $$\sin x&lt;x&lt;\sin x+1-\cos x$$ The lower bound is clear because the $\sin x$ is the length of a straight segment and $x$ is the length of a curved segment with the same endpoints. But I find that the upper bound is based on this intuitive fact:     If $F$ and $G$ are two convex subsets of $\Bbb R^2$ and   $F\subset G$, then $|\partial F|&lt;|\partial G|$.   ($|\partial F|$ is the length of the boundary of $F$).  But I haven't ever seen a proof of that. I tried it myself but I get stuck trying to bound $$\int_s^t\sqrt{1+y'(u)^2}du$$ provided for example that $y''$ is negative and $y(s)=y(t)$. I realize that $y'$ can't be bounded (note for example $y=\sqrt{1-x^2}$, $-1\le x\le 1$).  Question  Is it possible to justify the derivative of $\sin x$ or the inequality about the lengths of bounds? Is there any calculus text with this approach (or similar) to define trigonometical functions? 
A.96	Let $\sum_i a_i$ be a convergent sum with positive $a_i$. Does $\sum_i \frac{a_i}{a_i+a_{i+1}+a_{i+2}+\cdots}$ always diverge?. Let $\sum_i a_i$ be a convergent sum, with all $a_i$ positive.  Let $s_n=\sum_{i=n}^{\infty}a_i$.      Does $\sum_i  \frac{a_i}{s_i}$ always diverge?   I've tried a few examples such as $a_i= r^i$ (geometric series) and $a_i=1/i^2$ and it seems to always diverge. 
A.97	what is the dimension of $\mathbb{R}$ at a vector space over there field $\mathbb{Q}$?. If we look at $\mathbb{C}$ as a vector space over $\mathbb{R}$ it's dimension will be $2$, because $\mathbb{C} = span\{1,i\}$.  A question I thought of is what would be the dimension of $\mathbb{R}$ as a vector space over $\mathbb{Q}$?  I feel like the answer should be infinity, because if the dimension was finite, say $n$, then for every $m := n+1$ real numbers  $x_1,...x_m$ there was a linear combination with rational coefficients that gives $0$: $\frac{a_1}{b_1}x_1+...+\frac{a_m}{b_m}x_m = 0$. Multiplying by $lcm(b_1,...,b_m)$ we get that for every $m$ real numbers there is a linear combination with natural coefficients that gives $0$. That feels false, how do you prove it? 
A.98	If $R:S^{1}\rightarrow S^{1}$ is a irrational rotation, $\{R^{n}([x])\}$ is dense in $S^{1}$ for all points.. Let $\alpha$ a irrational number, and $R:S^{1}\rightarrow S^{1}$ the irrational rotation, i.e., $[x]\rightarrow[x+\alpha]$. I need to prove that, for all $[x]\in S^{1}$, the set $\{R^{n}([x])\}$ is dense in $S^{1}$.  First, I can write $R$ by  $$R(e^{2\pi ix})=e^{2\pi i (x+\alpha)}.$$  So, I can write $R^{n}(x)$, $n\in\mathbb{Z}$ by  $$R^{n}(e^{2\pi i x})=e^{2\pi i(x+n\alpha)} $$  I need to prove that, for all $e^{2\pi i x}\in S^{1}$ and for all $[y]=e^{2\pi i y}\in S^{1}$, every neighborhood $V$ of $[y]$ contains a point $[z_{y}]=e^{2\pi i z}$ such that $[z_y]=R^{n}([x])$ for some $n\in\mathbb{Z}$. That is,  $e^{2\pi i z}=e^{2\pi i (x+\alpha n)}\Rightarrow 2\pi iz=2\pi i(x+\alpha n)+2ik\pi\;\textrm{for some}\;k\in\mathbb{Z}\Rightarrow z=x+\alpha n+k.$  Is my way correct? If it does, how can I proceed now? If it doesn't, what I need to do?  I don't think this question is duplicate. I'm showing my attempt to proof, that is different from other proofs. 
A.99	Rationals can be the set of continuity of a function?. Most of the functions that I have seen have their discontinuities on rationals and continuities on irrationals!  I am wondering if there is any exampe of some function whose continuities are rationals? Or is other words      The set of continuities of a function $f:\mathbb{R}\to\mathbb{R}$ can be $\mathbb{Q}$?  
A.100	Is this set "not closed"?. Is it correct to say that this set $E=(0,1]$ where $E\subseteq R$ (Where $R$ is the set of real numbers) is not closed? 
